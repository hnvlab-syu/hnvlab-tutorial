{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemented and written by Yeoreum Lee in AI HnV Lab @ Sahmyook University in 2023\n",
    "__author__ = 'leeyeoreum02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 불러오기\n",
    "\n",
    "메인 교재와 서브 강의만을 활용하여 pandas로 csv 파일에 저장되어 있는 MNIST 데이터셋을 불러오시오. (구글링 금지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (10000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      5    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      9    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/mnist_train.csv')\n",
    "test_data = pd.read_csv('data/mnist_test.csv')\n",
    "\n",
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (10000, 785)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13341</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48698</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31206</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37930</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40323</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "13341      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "48698      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "31206      8    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "37930      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "40323      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "       28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "13341      0      0      0      0      0      0      0      0  \n",
       "48698      0      0      0      0      0      0      0      0  \n",
       "31206      0      0      0      0      0      0      0      0  \n",
       "37930      0      0      0      0      0      0      0      0  \n",
       "40323      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle\n",
    "train_data = train_data.sample(frac=1)\n",
    "\n",
    "print(train_data.shape, test_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 나누기(split)\n",
    "\n",
    "메인 교재와 서브 강의만을 이용하여 data를 feature와 정답(label)로 나누는 코드를 구현하시오. (구글링 금지)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def split_data(train_data: pd.DataFrame, test_data: pd.DataFrame) -> Tuple[np.ndarray]:\n",
    "    test_x_data = np.array(test_data.iloc[:, 1:])\n",
    "    test_t_data = np.array(test_data.iloc[:, 0])\n",
    "    train_x_data = np.array(train_data.iloc[:, 1:])\n",
    "    train_t_data = np.array(train_data.iloc[:, 0])\n",
    "    \n",
    "    return train_x_data, train_t_data, test_x_data, test_t_data\n",
    "\n",
    "\n",
    "train_x_data, train_t_data, test_x_data, test_t_data = split_data(train_data, test_data)\n",
    "print(train_x_data.shape, train_t_data.shape, test_x_data.shape, test_t_data.shape,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 데이터 전처리(preprocessing)\n",
    "\n",
    "a. 메인 교재와 서브 강의를 활용하여 train data를 normalize하시오. (구글링 가능)\n",
    "\n",
    "$$normalize(X) = \\frac {X - min} {max - min} \\space (max - min \\neq 0)$$\n",
    "\n",
    "b. 메인 교재와 서브 강의를 활용하여 test data를 one-hot encoding하시오. (구글링 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.99 0.01]\n",
      " [0.01 0.01 0.99 0.01 0.01 0.01 0.01 0.01 0.01 0.01]\n",
      " [0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.99 0.01]\n",
      " [0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.99 0.01 0.01]\n",
      " [0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.99 0.01 0.01]]\n",
      "(60000, 784) (60000, 10) (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(train_x_data: np.ndarray, train_t_data: np.ndarray, test_x_data: np.ndarray, test_t_data: np.ndarray) -> Tuple[np.ndarray]:\n",
    "    train_x_data = np.array(train_x_data) / 255.0  # 0~1 min-max scaling (normalization)\n",
    "    train_x_data = train_x_data * 0.99 + 0.01  # smoothing\n",
    "    \n",
    "    test_x_data = np.array(test_x_data) / 255.0  # 0~1 min-max scaling (normalization)\n",
    "    test_x_data = test_x_data * 0.99 + 0.01  # smoothing\n",
    "\n",
    "    num_classes = 10  # 0~9\n",
    "    train_t_data_onehot = np.zeros((train_t_data.shape[0], num_classes), dtype=np.float32) + 0.01  # one-hot encoding (vectorization) + smoothing\n",
    "    for i in range(len(train_t_data_onehot)):\n",
    "        train_t_data_onehot[i, train_t_data[i]] = 0.99  # smoothing\n",
    "\n",
    "    test_t_data_onehot = np.zeros((test_t_data.shape[0], num_classes), dtype=np.float32) + 0.01  # one-hot encoding (vectorization) + smoothing\n",
    "    for i in range(len(test_t_data_onehot)):\n",
    "        test_t_data_onehot[i, test_t_data[i]] = 0.99  # smoothing\n",
    "    \n",
    "    return train_x_data, train_t_data_onehot, test_x_data, test_t_data_onehot\n",
    "\n",
    "    \n",
    "train_x_data, train_t_data_onehot, test_x_data, test_t_data_onehot = preprocess(train_x_data, train_t_data, test_x_data, test_t_data)\n",
    "print(train_t_data_onehot[:5])\n",
    "print(train_x_data.shape, train_t_data_onehot.shape, test_x_data.shape, test_t_data_onehot.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 활성 함수(activation function)\n",
    "\n",
    "메인 교재와 서브 강의만을 이용하여 활성 함수 중 하나인 softmax 함수를 구현하시오. (구글링 금지)\n",
    "\n",
    "$$\\sigma(\\boldsymbol{z})_{i} = \\frac {e^{z_{i}}} {\\sum_{i=1} ^K e^{z_{i}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax(z: np.ndarray) -> np.ndarray:\n",
    "    return np.exp(z) / np.sum(np.exp(z))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 신경망(neural network) 모델\n",
    "\n",
    "메인 교재와 서브 강의만을 이용하여 신경망(neural network) 모델을 구현하시오. (구글링 금지)\n",
    "\n",
    "$$f(W^{(2)}, b^{(2)})(\\boldsymbol{x}) = sigmoid(W^{(2)}\\boldsymbol{x} + b^{(2)})$$\n",
    "$$g(W^{(3)}, b^{(3)})(\\boldsymbol{x}) = \\sigma(W^{(3)}\\boldsymbol{x} + b^{(3)})$$\n",
    "$$\n",
    "\\begin{matrix}\n",
    "y &=& h(W^{(2)}, b^{(2)}, W^{(3)}, b^{(3)})(\\boldsymbol{x}) \\\\\n",
    "  &=& (g(W^{(3)}, b^{(3)}) \\circ f(W^{(2)}, b^{(2)}))(\\boldsymbol{x}) \\\\\n",
    "  &=& g(W^{(3)}, b^{(3)})(f(W^{(2)}, b^{(2)})(\\boldsymbol{x})) \\\\\n",
    "  &=& \\sigma(W^{(3)}sigmoid(W^{(2)}\\boldsymbol{x} + b^{(2)}) + b^{(3)})\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, n_input: int, n_output: int, n_hidden: int = 128) -> None:\n",
    "        self.W2 = np.random.randn(n_input, n_hidden)  # he initialization\n",
    "        self.b2 = np.random.randn(n_hidden)\n",
    "        self.W3 = np.random.randn(n_hidden, n_output)\n",
    "        self.b3 = np.random.randn(n_output)\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        a1 = x\n",
    "        z2 = a1 @ self.W2 + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "        z3 = a2 @ self.W3 + self.b3\n",
    "        y = a3 = softmax(z3)\n",
    "        return y\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(x)\n",
    "    \n",
    "    \n",
    "model = NeuralNetwork(n_input=784, n_output=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 오차 함수 (error function, loss function)\n",
    "\n",
    "주어진 수식만을 이용하여 CE(cross entropy) 오차 함수를 구현하시오. (구글링, 메인 교재 참고, 서브 강의 참고 금지)\n",
    "- delta는 log의 진수 조건을 만족하기 위해 필요하므로 반드시 사용\n",
    "- N은 데이터 개수 (행 개수)\n",
    "- $y$는 정답(label) $\\hat{y}$은 예측값(prediction)\n",
    "\n",
    "$$CE = -\\sum_{i=1} ^N (\\boldsymbol{y_{i}} \\cdot \\log \\boldsymbol{\\hat{y_{i}}} + (1 - \\boldsymbol{y_{i}}) \\cdot \\log (1 - \\boldsymbol{\\hat{y_{i}}}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_data: np.ndarray, t_data: np.ndarray) -> np.ndarray:\n",
    "    delta = 1e-4\n",
    "    return -np.sum(t_data * np.log(y_data + delta) + (1 - t_data) * np.log((1 - y_data) + delta))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 편미분 함수 (partial numerical derivative)\n",
    "\n",
    "- n은 input node 개수, m은 hidden node 개수, o는 output node 개수\n",
    "\n",
    "$$J(W^{(2)}) = \\frac{dE} {dW^{(2)}} = \\begin{pmatrix} \n",
    "\\frac{\\partial E} {\\partial W^{(2)}_{11}} & \\cdots & \\frac {\\partial E} {\\partial W^{(2)}_{m1}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "\\frac {\\partial E} {\\partial W^{(2)}_{1n}} & \\cdots & \\frac {\\partial E} {\\partial W^{(2)}_{mn}} \\end{pmatrix}$$\n",
    "\n",
    "$$J(W^{(3)}) = \\frac{dE} {dW^{(3)}} = \\begin{pmatrix} \n",
    "\\frac{\\partial E} {\\partial W^{(3)}_{11}} & \\cdots & \\frac {\\partial E} {\\partial W^{(3)}_{o1}} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\ \n",
    "\\frac {\\partial E} {\\partial W^{(3)}_{1m}} & \\cdots & \\frac {\\partial E} {\\partial W^{(3)}_{om}} \\end{pmatrix}$$\n",
    "\n",
    "$$J(\\boldsymbol{b^{(2)}}) = \\frac{dE} {d\\boldsymbol{b^{(2)}}} = \\begin{pmatrix} \\frac{\\partial E} {\\partial b^{(2)}_{1}} \\frac{\\partial E} {\\partial b^{(2)}_{2}} \\end{pmatrix}$$\n",
    "\n",
    "$$J(\\boldsymbol{b^{(3)}}) = \\frac{dE} {d\\boldsymbol{b^{(3)}}} = \\begin{pmatrix} \\frac{\\partial E} {\\partial b^{(3)}_{1}} \\frac{\\partial E} {\\partial b^{(3)}_{2}} \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f: Callable, x: np.ndarray) -> np.ndarray:\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        \n",
    "        temp = x[idx]\n",
    "        x[idx] = float(temp) + h\n",
    "        fx1 = f(x)\n",
    "        \n",
    "        x[idx] = temp - h\n",
    "        fx2 = f(x)\n",
    "        grad[idx] = (fx1 - fx2) / (2 * h)\n",
    "        \n",
    "        x[idx] = temp\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-1. 모델 학습 (train)\n",
    "\n",
    "메인 교재와 서브 강의만을 이용하여 다음 순서를 가지는 학습 코드를 구현하시오. (구글링 금지)\n",
    "- 한꺼번에 다 학습 -> batch(뭉텅이) gradient descent\n",
    "\n",
    "1. 모델 순전파 (forward)\n",
    "2. 오차 계산 (loss)\n",
    "3. 모델 파라미터(가중치 + 편향) 별 오차 함수의 편미분값 계산 (numerical derivative)\n",
    "4. 가중치(weight), 편향(bias) 갱신 (경사 하강법, gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m         model\u001b[39m.\u001b[39mb3 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m numerical_derivative(E_b3, model\u001b[39m.\u001b[39mb3)\n\u001b[1;32m     21\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, loss \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m train_batch()\n",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m E_w3 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W3: cross_entropy_loss(h(model\u001b[39m.\u001b[39mW2, model\u001b[39m.\u001b[39mb2, W3, model\u001b[39m.\u001b[39mb3), train_t_data_onehot)\n\u001b[1;32m     14\u001b[0m E_b3 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m b3: cross_entropy_loss(h(model\u001b[39m.\u001b[39mW2, model\u001b[39m.\u001b[39mb2, model\u001b[39m.\u001b[39mW3, b3), train_t_data_onehot)\n\u001b[0;32m---> 16\u001b[0m model\u001b[39m.\u001b[39mW2 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m numerical_derivative(E_w2, model\u001b[39m.\u001b[39;49mW2)\n\u001b[1;32m     17\u001b[0m model\u001b[39m.\u001b[39mb2 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m numerical_derivative(E_b2, model\u001b[39m.\u001b[39mb2)\n\u001b[1;32m     18\u001b[0m model\u001b[39m.\u001b[39mW3 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m numerical_derivative(E_w3, model\u001b[39m.\u001b[39mW3)\n",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m, in \u001b[0;36mnumerical_derivative\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m temp \u001b[39m=\u001b[39m x[idx]\n\u001b[1;32m     11\u001b[0m x[idx] \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(temp) \u001b[39m+\u001b[39m h\n\u001b[0;32m---> 12\u001b[0m fx1 \u001b[39m=\u001b[39m f(x)\n\u001b[1;32m     14\u001b[0m x[idx] \u001b[39m=\u001b[39m temp \u001b[39m-\u001b[39m h\n\u001b[1;32m     15\u001b[0m fx2 \u001b[39m=\u001b[39m f(x)\n",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m, in \u001b[0;36mtrain_batch.<locals>.<lambda>\u001b[0;34m(W2)\u001b[0m\n\u001b[1;32m      8\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2, b2: sigmoid(train_x_data \u001b[39m@\u001b[39m W2 \u001b[39m+\u001b[39m b2)  \u001b[39m# f(W2, b2)(x) = sigmoid(W2x + b2)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2, b2, W3, b3: softmax(f(W2, b2) \u001b[39m@\u001b[39m W3 \u001b[39m+\u001b[39m b3)  \u001b[39m# y = h(W2, b2, W3, b3)(x) = g(W3, b3)(f(W2, b2)(x)) = softmax(W3(sigmoid(W3x + b2)) + b3)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m E_w2 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2: cross_entropy_loss(h(W2, model\u001b[39m.\u001b[39;49mb2, model\u001b[39m.\u001b[39;49mW3, model\u001b[39m.\u001b[39;49mb3), train_t_data_onehot)\n\u001b[1;32m     12\u001b[0m E_b2 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m b2: cross_entropy_loss(h(model\u001b[39m.\u001b[39mW2, b2, model\u001b[39m.\u001b[39mW3, model\u001b[39m.\u001b[39mb3), train_t_data_onehot)\n\u001b[1;32m     13\u001b[0m E_w3 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W3: cross_entropy_loss(h(model\u001b[39m.\u001b[39mW2, model\u001b[39m.\u001b[39mb2, W3, model\u001b[39m.\u001b[39mb3), train_t_data_onehot)\n",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m, in \u001b[0;36mtrain_batch.<locals>.<lambda>\u001b[0;34m(W2, b2, W3, b3)\u001b[0m\n\u001b[1;32m      6\u001b[0m loss \u001b[39m=\u001b[39m cross_entropy_loss(y_data, train_t_data_onehot)\n\u001b[1;32m      8\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2, b2: sigmoid(train_x_data \u001b[39m@\u001b[39m W2 \u001b[39m+\u001b[39m b2)  \u001b[39m# f(W2, b2)(x) = sigmoid(W2x + b2)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2, b2, W3, b3: softmax(f(W2, b2) \u001b[39m@\u001b[39m W3 \u001b[39m+\u001b[39m b3)  \u001b[39m# y = h(W2, b2, W3, b3)(x) = g(W3, b3)(f(W2, b2)(x)) = softmax(W3(sigmoid(W3x + b2)) + b3)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m E_w2 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2: cross_entropy_loss(h(W2, model\u001b[39m.\u001b[39mb2, model\u001b[39m.\u001b[39mW3, model\u001b[39m.\u001b[39mb3), train_t_data_onehot)\n\u001b[1;32m     12\u001b[0m E_b2 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m b2: cross_entropy_loss(h(model\u001b[39m.\u001b[39mW2, b2, model\u001b[39m.\u001b[39mW3, model\u001b[39m.\u001b[39mb3), train_t_data_onehot)\n",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m, in \u001b[0;36mtrain_batch.<locals>.<lambda>\u001b[0;34m(W2, b2)\u001b[0m\n\u001b[1;32m      5\u001b[0m y_data \u001b[39m=\u001b[39m model(train_x_data)\n\u001b[1;32m      6\u001b[0m loss \u001b[39m=\u001b[39m cross_entropy_loss(y_data, train_t_data_onehot)\n\u001b[0;32m----> 8\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2, b2: sigmoid(train_x_data \u001b[39m@\u001b[39;49m W2 \u001b[39m+\u001b[39m b2)  \u001b[39m# f(W2, b2)(x) = sigmoid(W2x + b2)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2, b2, W3, b3: softmax(f(W2, b2) \u001b[39m@\u001b[39m W3 \u001b[39m+\u001b[39m b3)  \u001b[39m# y = h(W2, b2, W3, b3)(x) = g(W3, b3)(f(W2, b2)(x)) = softmax(W3(sigmoid(W3x + b2)) + b3)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m E_w2 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2: cross_entropy_loss(h(W2, model\u001b[39m.\u001b[39mb2, model\u001b[39m.\u001b[39mW3, model\u001b[39m.\u001b[39mb3), train_t_data_onehot)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_batch() -> None:\n",
    "    lr = 1e-2\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        y_data = model(train_x_data)\n",
    "        loss = cross_entropy_loss(y_data, train_t_data_onehot)\n",
    "\n",
    "        f = lambda W2, b2: sigmoid(train_x_data @ W2 + b2)  # f(W2, b2)(x) = sigmoid(W2x + b2)\n",
    "        h = lambda W2, b2, W3, b3: softmax(f(W2, b2) @ W3 + b3)  # y = h(W2, b2, W3, b3)(x) = g(W3, b3)(f(W2, b2)(x)) = softmax(W3(sigmoid(W3x + b2)) + b3)\n",
    "        \n",
    "        E_w2 = lambda W2: cross_entropy_loss(h(W2, model.b2, model.W3, model.b3), train_t_data_onehot)\n",
    "        E_b2 = lambda b2: cross_entropy_loss(h(model.W2, b2, model.W3, model.b3), train_t_data_onehot)\n",
    "        E_w3 = lambda W3: cross_entropy_loss(h(model.W2, model.b2, W3, model.b3), train_t_data_onehot)\n",
    "        E_b3 = lambda b3: cross_entropy_loss(h(model.W2, model.b2, model.W3, b3), train_t_data_onehot)\n",
    "        \n",
    "        model.W2 -= lr * numerical_derivative(E_w2, model.W2)\n",
    "        model.b2 -= lr * numerical_derivative(E_b2, model.b2)\n",
    "        model.W3 -= lr * numerical_derivative(E_w3, model.W3)\n",
    "        model.b3 -= lr * numerical_derivative(E_b3, model.b3)\n",
    "\n",
    "        print(f'Epoch: {epoch}, loss {loss}')\n",
    "\n",
    "\n",
    "train_batch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-2. 모델 학습 (train)\n",
    "\n",
    "메인 교재와 서브 강의만을 이용하여 다음 순서를 가지는 학습 코드를 구현하시오. (구글링 금지)\n",
    "- mini-batch 단위로 학습 -> mini-batch gradient descent\n",
    "- 만약 batch size가 1인 경우 -> stocastic gradient descent\n",
    "- 메모리가 부족해 한꺼번에 다 안들어갈 경우 위의 방식들을 사용\n",
    "- 여기선 메모리가 부족하진 않지만 loss 변화가 너무 늦게 출력되니까 사용\n",
    "\n",
    "1. 모델 순전파 (forward)\n",
    "2. 오차 계산 (loss)\n",
    "3. 모델 파라미터(가중치 + 편향) 별 오차 함수의 편미분값 계산 (numerical derivative)\n",
    "4. 가중치(weight), 편향(bias) 갱신 (경사 하강법, gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, step: 0, loss 15.511046708496744\n",
      "Epoch: 0, step: 1, loss 14.832540182267893\n",
      "Epoch: 0, step: 2, loss 14.159318050945908\n",
      "Epoch: 0, step: 3, loss 16.963974420043456\n",
      "Epoch: 0, step: 4, loss 15.270109633053956\n",
      "Epoch: 0, step: 5, loss 11.896099195846023\n",
      "Epoch: 0, step: 6, loss 12.174195559059925\n",
      "Epoch: 0, step: 7, loss 17.550386223579913\n",
      "Epoch: 0, step: 8, loss 18.25124486926252\n",
      "Epoch: 0, step: 9, loss 12.393886603924248\n",
      "Epoch: 0, step: 10, loss 10.784711657983447\n",
      "Epoch: 0, step: 11, loss 10.039738444914\n",
      "Epoch: 0, step: 12, loss 11.053194409087952\n",
      "Epoch: 0, step: 13, loss 13.270621767406599\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     27\u001b[0m                 \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, step: \u001b[39m\u001b[39m{\u001b[39;00mstep\u001b[39m}\u001b[39;00m\u001b[39m, loss \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m train_mini_batch()\n",
      "Cell \u001b[0;32mIn[12], line 21\u001b[0m, in \u001b[0;36mtrain_mini_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m E_w3 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W3: cross_entropy_loss(h(model\u001b[39m.\u001b[39mW2, model\u001b[39m.\u001b[39mb2, W3, model\u001b[39m.\u001b[39mb3), t_batch)\n\u001b[1;32m     19\u001b[0m E_b3 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m b3: cross_entropy_loss(h(model\u001b[39m.\u001b[39mW2, model\u001b[39m.\u001b[39mb2, model\u001b[39m.\u001b[39mW3, b3), t_batch)\n\u001b[0;32m---> 21\u001b[0m model\u001b[39m.\u001b[39mW2 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m numerical_derivative(E_w2, model\u001b[39m.\u001b[39;49mW2)\n\u001b[1;32m     22\u001b[0m model\u001b[39m.\u001b[39mb2 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m numerical_derivative(E_b2, model\u001b[39m.\u001b[39mb2)\n\u001b[1;32m     23\u001b[0m model\u001b[39m.\u001b[39mW3 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m lr \u001b[39m*\u001b[39m numerical_derivative(E_w3, model\u001b[39m.\u001b[39mW3)\n",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m, in \u001b[0;36mnumerical_derivative\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m temp \u001b[39m=\u001b[39m x[idx]\n\u001b[1;32m     11\u001b[0m x[idx] \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(temp) \u001b[39m+\u001b[39m h\n\u001b[0;32m---> 12\u001b[0m fx1 \u001b[39m=\u001b[39m f(x)\n\u001b[1;32m     14\u001b[0m x[idx] \u001b[39m=\u001b[39m temp \u001b[39m-\u001b[39m h\n\u001b[1;32m     15\u001b[0m fx2 \u001b[39m=\u001b[39m f(x)\n",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m, in \u001b[0;36mtrain_mini_batch.<locals>.<lambda>\u001b[0;34m(W2)\u001b[0m\n\u001b[1;32m     13\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2, b2: sigmoid(x_batch \u001b[39m@\u001b[39m W2 \u001b[39m+\u001b[39m b2)  \u001b[39m# f(W2, b2)(x) = sigmoid(W2x + b2)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2, b2, W3, b3: softmax(f(W2, b2) \u001b[39m@\u001b[39m W3 \u001b[39m+\u001b[39m b3)  \u001b[39m# y = h(W2, b2, W3, b3)(x) = g(W3, b3)(f(W2, b2)(x)) = softmax(W3(sigmoid(W3x + b2)) + b3)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m E_w2 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2: cross_entropy_loss(h(W2, model\u001b[39m.\u001b[39;49mb2, model\u001b[39m.\u001b[39;49mW3, model\u001b[39m.\u001b[39;49mb3), t_batch)\n\u001b[1;32m     17\u001b[0m E_b2 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m b2: cross_entropy_loss(h(model\u001b[39m.\u001b[39mW2, b2, model\u001b[39m.\u001b[39mW3, model\u001b[39m.\u001b[39mb3), t_batch)\n\u001b[1;32m     18\u001b[0m E_w3 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W3: cross_entropy_loss(h(model\u001b[39m.\u001b[39mW2, model\u001b[39m.\u001b[39mb2, W3, model\u001b[39m.\u001b[39mb3), t_batch)\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mtrain_mini_batch.<locals>.<lambda>\u001b[0;34m(W2, b2, W3, b3)\u001b[0m\n\u001b[1;32m     11\u001b[0m loss \u001b[39m=\u001b[39m cross_entropy_loss(y_data, t_batch)\n\u001b[1;32m     13\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2, b2: sigmoid(x_batch \u001b[39m@\u001b[39m W2 \u001b[39m+\u001b[39m b2)  \u001b[39m# f(W2, b2)(x) = sigmoid(W2x + b2)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2, b2, W3, b3: softmax(f(W2, b2) \u001b[39m@\u001b[39m W3 \u001b[39m+\u001b[39m b3)  \u001b[39m# y = h(W2, b2, W3, b3)(x) = g(W3, b3)(f(W2, b2)(x)) = softmax(W3(sigmoid(W3x + b2)) + b3)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m E_w2 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2: cross_entropy_loss(h(W2, model\u001b[39m.\u001b[39mb2, model\u001b[39m.\u001b[39mW3, model\u001b[39m.\u001b[39mb3), t_batch)\n\u001b[1;32m     17\u001b[0m E_b2 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m b2: cross_entropy_loss(h(model\u001b[39m.\u001b[39mW2, b2, model\u001b[39m.\u001b[39mW3, model\u001b[39m.\u001b[39mb3), t_batch)\n",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m, in \u001b[0;36mtrain_mini_batch.<locals>.<lambda>\u001b[0;34m(W2, b2)\u001b[0m\n\u001b[1;32m     10\u001b[0m y_data \u001b[39m=\u001b[39m model(x_batch)\n\u001b[1;32m     11\u001b[0m loss \u001b[39m=\u001b[39m cross_entropy_loss(y_data, t_batch)\n\u001b[0;32m---> 13\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2, b2: sigmoid(x_batch \u001b[39m@\u001b[39;49m W2 \u001b[39m+\u001b[39m b2)  \u001b[39m# f(W2, b2)(x) = sigmoid(W2x + b2)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2, b2, W3, b3: softmax(f(W2, b2) \u001b[39m@\u001b[39m W3 \u001b[39m+\u001b[39m b3)  \u001b[39m# y = h(W2, b2, W3, b3)(x) = g(W3, b3)(f(W2, b2)(x)) = softmax(W3(sigmoid(W3x + b2)) + b3)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m E_w2 \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m W2: cross_entropy_loss(h(W2, model\u001b[39m.\u001b[39mb2, model\u001b[39m.\u001b[39mW3, model\u001b[39m.\u001b[39mb3), t_batch)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_mini_batch() -> None:\n",
    "    lr = 1e-3\n",
    "    batch_size = 1\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        for step in range(len(train_x_data) // batch_size):\n",
    "            x_batch = train_x_data[step*batch_size:(step+1)*batch_size]\n",
    "            t_batch = train_t_data_onehot[step*batch_size:(step+1)*batch_size]\n",
    "            \n",
    "            y_data = model(x_batch)\n",
    "            loss = cross_entropy_loss(y_data, t_batch)\n",
    "\n",
    "            f = lambda W2, b2: sigmoid(x_batch @ W2 + b2)  # f(W2, b2)(x) = sigmoid(W2x + b2)\n",
    "            h = lambda W2, b2, W3, b3: softmax(f(W2, b2) @ W3 + b3)  # y = h(W2, b2, W3, b3)(x) = g(W3, b3)(f(W2, b2)(x)) = softmax(W3(sigmoid(W3x + b2)) + b3)\n",
    "            \n",
    "            E_w2 = lambda W2: cross_entropy_loss(h(W2, model.b2, model.W3, model.b3), t_batch)\n",
    "            E_b2 = lambda b2: cross_entropy_loss(h(model.W2, b2, model.W3, model.b3), t_batch)\n",
    "            E_w3 = lambda W3: cross_entropy_loss(h(model.W2, model.b2, W3, model.b3), t_batch)\n",
    "            E_b3 = lambda b3: cross_entropy_loss(h(model.W2, model.b2, model.W3, b3), t_batch)\n",
    "            \n",
    "            model.W2 -= lr * numerical_derivative(E_w2, model.W2)\n",
    "            model.b2 -= lr * numerical_derivative(E_b2, model.b2)\n",
    "            model.W3 -= lr * numerical_derivative(E_w3, model.W3)\n",
    "            model.b3 -= lr * numerical_derivative(E_b3, model.b3)\n",
    "\n",
    "            if step % 1 == 0:\n",
    "                print(f'Epoch: {epoch}, step: {step}, loss {loss}')\n",
    "                \n",
    "                \n",
    "train_mini_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2185028b62590b62afdaea33c23835374a0efabc80ef4ce750ba82ee2e8657e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
