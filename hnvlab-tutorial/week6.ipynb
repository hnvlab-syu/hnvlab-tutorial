{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemented and written by Yeoreum Lee in AI HnV Lab @ Sahmyook University in 2023\n",
    "__author__ = 'leeyeoreum02'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 컨볼루션 계산\n",
    "\n",
    "서브 강의를 활용하여 다음 예제들의 정답을 손으로 계산하시오. (구글링 금지)\n",
    "- *은 컨볼루션 연산 기호\n",
    "- +은 브로드캐스트가 적용된 더하기 기호\n",
    "\n",
    "1. \n",
    "$$\\begin{pmatrix} 0 & 2 & 2 & 2 & 1 \\\\ 0 & 2 & 1 & 2 & 1 \\\\ 0 & 1 & 0 & 2 & 1 \\\\ 0 & 0 & 0 & 2 & 1 \\\\ 0 & 0 & 0 & 2 & 1 \\end{pmatrix} * \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}$$\n",
    "\n",
    "2.\n",
    "$$\\begin{pmatrix} 0 & 2 & 2 & 2 & 1 \\\\ 0 & 2 & 1 & 2 & 1 \\\\ 0 & 1 & 0 & 2 & 1 \\\\ 0 & 0 & 0 & 2 & 1 \\\\ 0 & 0 & 0 & 2 & 1 \\end{pmatrix} * \\begin{pmatrix} -1 & -1 & -1 \\\\ 2 & 2 & 2 \\\\ -1 & -1 & -1 \\end{pmatrix} + \\begin{pmatrix} 1 \\end{pmatrix}$$\n",
    "\n",
    "3.\n",
    "$$\\begin{pmatrix} 0 & 2 & 2 & 2 & 1 \\\\ 0 & 2 & 1 & 2 & 1 \\\\ 0 & 1 & 0 & 2 & 1 \\\\ 0 & 0 & 0 & 2 & 1 \\\\ 0 & 0 & 0 & 2 & 1 \\end{pmatrix} * \\begin{pmatrix} -1 & 2 & -1 \\\\ -1 & 2 & -1 \\\\ -1 & 2 & -1 \\end{pmatrix} + \\begin{pmatrix} -1 \\end{pmatrix}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 파이토치 (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2]) torch.Size([9, 1])\n"
     ]
    }
   ],
   "source": [
    "x_data = torch.tensor([[2, 4], [4, 11], [6, 6], [8, 5], [10, 7], [12, 16], [14, 8], [16, 3], [18, 7]])\n",
    "t_data = torch.tensor([0, 0, 0, 0, 1, 1, 1, 1, 1]).view(9, 1)\n",
    "\n",
    "print(x_data.shape, t_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 데이터 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 2]) torch.Size([8, 1]) torch.Size([1, 2]) torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "def split_data(x_data: torch.Tensor, t_data: torch.Tensor, split_rate: float) -> Tuple[torch.Tensor]:\n",
    "    test_x_data = x_data[:int(split_rate * len(x_data))]\n",
    "    test_t_data = t_data[:int(split_rate * len(t_data))]\n",
    "    train_x_data = x_data[int(split_rate * len(x_data)):]\n",
    "    train_t_data = t_data[int(split_rate * len(t_data)):]\n",
    "    \n",
    "    return train_x_data, train_t_data, test_x_data, test_t_data\n",
    "\n",
    "train_x_data, train_t_data, test_x_data, test_t_data = split_data(x_data, t_data, split_rate=0.2)\n",
    "print(train_x_data.shape, train_t_data.shape, test_x_data.shape, test_t_data.shape,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 원핫 인코딩(One-hot Encoding)\n",
    "\n",
    "파이토치 문서(document)를 활용하여 test data를 one-hot encoding하시오. (구글링 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [0, 1],\n",
      "        [0, 1]])\n",
      "torch.Size([8, 2]) torch.Size([8, 2]) torch.Size([1, 2]) torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "train_t_data_onehot = F.one_hot(train_t_data).squeeze(1)\n",
    "test_t_data_onehot = ...\n",
    "print(train_t_data_onehot[:5])\n",
    "print(train_x_data.shape, train_t_data_onehot.shape, test_x_data.shape, test_t_data_onehot.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. 신경망(neural network) 모델\n",
    "\n",
    "파이토치 튜토리얼과 week5.ipynb를 참고하여 파이토치로 신경망 (neural network) 모델을 구현하시오. (구글링 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.W2_b2 = nn.Linear(2, 2)\n",
    "        self.W3_b3 = ...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        a1 = x\n",
    "        z2 = ...\n",
    "        a2 = torch.sigmoid(z2)\n",
    "        z3 = ...\n",
    "        y = a3 = torch.sigmoid(z3)\n",
    "        return y\n",
    "\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-4. 모델 학습 (train)\n",
    "\n",
    "파이토치 튜토리얼과 week5.ipynb를 참고하여 다음 순서를 가지는 학습 코드를 구현하시오. (구글링 가능)\n",
    "\n",
    "0. 배치 사이즈는 1임\n",
    "1. 모델 순전파 (forward)\n",
    "2. 오차 계산 (loss)\n",
    "3. 모델 파라미터(가중치 + 편향) 별 오차 함수의 오차역전파 계산 (backpropagation)\n",
    "4. 가중치(weight), 편향(bias) 갱신 (경사 하강법, gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss 0.6571138501167297\n",
      "Epoch: 0, loss 0.34611445665359497\n",
      "Epoch: 0, loss 0.28464654088020325\n",
      "Epoch: 0, loss 0.5063809752464294\n",
      "Epoch: 0, loss 0.4651125371456146\n",
      "Epoch: 0, loss 0.520247220993042\n",
      "Epoch: 0, loss 0.6321440935134888\n",
      "Epoch: 0, loss 0.5477754473686218\n",
      "Epoch: 10, loss 0.6381607055664062\n",
      "Epoch: 10, loss 0.3524288237094879\n",
      "Epoch: 10, loss 0.29856687784194946\n",
      "Epoch: 10, loss 0.5040720701217651\n",
      "Epoch: 10, loss 0.4709720313549042\n",
      "Epoch: 10, loss 0.5148986577987671\n",
      "Epoch: 10, loss 0.6022799015045166\n",
      "Epoch: 10, loss 0.5334166288375854\n",
      "Epoch: 20, loss 0.6166685819625854\n",
      "Epoch: 20, loss 0.35370057821273804\n",
      "Epoch: 20, loss 0.3051862120628357\n",
      "Epoch: 20, loss 0.5037751793861389\n",
      "Epoch: 20, loss 0.47626522183418274\n",
      "Epoch: 20, loss 0.5127284526824951\n",
      "Epoch: 20, loss 0.5854319334030151\n",
      "Epoch: 20, loss 0.5268880128860474\n",
      "Epoch: 30, loss 0.5940252542495728\n",
      "Epoch: 30, loss 0.35203152894973755\n",
      "Epoch: 30, loss 0.3075602948665619\n",
      "Epoch: 30, loss 0.5044156908988953\n",
      "Epoch: 30, loss 0.48081350326538086\n",
      "Epoch: 30, loss 0.5119608044624329\n",
      "Epoch: 30, loss 0.5753151774406433\n",
      "Epoch: 30, loss 0.5237290859222412\n",
      "Epoch: 40, loss 0.5712184906005859\n",
      "Epoch: 40, loss 0.34846124053001404\n",
      "Epoch: 40, loss 0.3072632849216461\n",
      "Epoch: 40, loss 0.5055610537528992\n",
      "Epoch: 40, loss 0.4845874607563019\n",
      "Epoch: 40, loss 0.5119467377662659\n",
      "Epoch: 40, loss 0.5689017176628113\n",
      "Epoch: 40, loss 0.5222645401954651\n",
      "Epoch: 50, loss 0.5488739609718323\n",
      "Epoch: 50, loss 0.34354260563850403\n",
      "Epoch: 50, loss 0.3051697015762329\n",
      "Epoch: 50, loss 0.5070202350616455\n",
      "Epoch: 50, loss 0.48766031861305237\n",
      "Epoch: 50, loss 0.5124142169952393\n",
      "Epoch: 50, loss 0.5646381378173828\n",
      "Epoch: 50, loss 0.5217680335044861\n",
      "Epoch: 60, loss 0.5272666215896606\n",
      "Epoch: 60, loss 0.33759158849716187\n",
      "Epoch: 60, loss 0.3018076419830322\n",
      "Epoch: 60, loss 0.5087091326713562\n",
      "Epoch: 60, loss 0.49016091227531433\n",
      "Epoch: 60, loss 0.5132373571395874\n",
      "Epoch: 60, loss 0.5616588592529297\n",
      "Epoch: 60, loss 0.5218806862831116\n",
      "Epoch: 70, loss 0.5063793063163757\n",
      "Epoch: 70, loss 0.33080369234085083\n",
      "Epoch: 70, loss 0.29752302169799805\n",
      "Epoch: 70, loss 0.5105950236320496\n",
      "Epoch: 70, loss 0.4922378659248352\n",
      "Epoch: 70, loss 0.5143534541130066\n",
      "Epoch: 70, loss 0.559440553188324\n",
      "Epoch: 70, loss 0.5224009156227112\n",
      "Epoch: 80, loss 0.48599645495414734\n",
      "Epoch: 80, loss 0.3233144283294678\n",
      "Epoch: 80, loss 0.2925635874271393\n",
      "Epoch: 80, loss 0.5126689672470093\n",
      "Epoch: 80, loss 0.49404144287109375\n",
      "Epoch: 80, loss 0.5157269835472107\n",
      "Epoch: 80, loss 0.5576419234275818\n",
      "Epoch: 80, loss 0.5231984257698059\n",
      "Epoch: 90, loss 0.4658031463623047\n",
      "Epoch: 90, loss 0.31523481011390686\n",
      "Epoch: 90, loss 0.2871246635913849\n",
      "Epoch: 90, loss 0.5149305462837219\n",
      "Epoch: 90, loss 0.49571868777275085\n",
      "Epoch: 90, loss 0.5173316597938538\n",
      "Epoch: 90, loss 0.5560244917869568\n",
      "Epoch: 90, loss 0.5241742134094238\n",
      "Epoch: 100, loss 0.4454694986343384\n",
      "Epoch: 100, loss 0.3066747486591339\n",
      "Epoch: 100, loss 0.28137820959091187\n",
      "Epoch: 100, loss 0.5173763036727905\n",
      "Epoch: 100, loss 0.49741673469543457\n",
      "Epoch: 100, loss 0.5191383957862854\n",
      "Epoch: 100, loss 0.5544120669364929\n",
      "Epoch: 100, loss 0.5252398252487183\n",
      "Epoch: 110, loss 0.42472055554389954\n",
      "Epoch: 110, loss 0.29776009917259216\n",
      "Epoch: 110, loss 0.2754916250705719\n",
      "Epoch: 110, loss 0.5199910998344421\n",
      "Epoch: 110, loss 0.49928906559944153\n",
      "Epoch: 110, loss 0.5211054682731628\n",
      "Epoch: 110, loss 0.5526692271232605\n",
      "Epoch: 110, loss 0.5263054966926575\n",
      "Epoch: 120, loss 0.4033924341201782\n",
      "Epoch: 120, loss 0.2886452078819275\n",
      "Epoch: 120, loss 0.26963984966278076\n",
      "Epoch: 120, loss 0.5227400660514832\n",
      "Epoch: 120, loss 0.501496434211731\n",
      "Epoch: 120, loss 0.5231710076332092\n",
      "Epoch: 120, loss 0.5506930947303772\n",
      "Epoch: 120, loss 0.5272749662399292\n",
      "Epoch: 130, loss 0.38147884607315063\n",
      "Epoch: 130, loss 0.27951815724372864\n",
      "Epoch: 130, loss 0.2640090882778168\n",
      "Epoch: 130, loss 0.5255627036094666\n",
      "Epoch: 130, loss 0.5041988492012024\n",
      "Epoch: 130, loss 0.5252488851547241\n",
      "Epoch: 130, loss 0.5484126210212708\n",
      "Epoch: 130, loss 0.528046190738678\n",
      "Epoch: 140, loss 0.3591519892215729\n",
      "Epoch: 140, loss 0.27059346437454224\n",
      "Epoch: 140, loss 0.25878944993019104\n",
      "Epoch: 140, loss 0.5283727645874023\n",
      "Epoch: 140, loss 0.5075339674949646\n",
      "Epoch: 140, loss 0.5272312760353088\n",
      "Epoch: 140, loss 0.5457953214645386\n",
      "Epoch: 140, loss 0.5285208821296692\n",
      "Epoch: 150, loss 0.33674925565719604\n",
      "Epoch: 150, loss 0.26209184527397156\n",
      "Epoch: 150, loss 0.2541557550430298\n",
      "Epoch: 150, loss 0.5310660600662231\n",
      "Epoch: 150, loss 0.5115840435028076\n",
      "Epoch: 150, loss 0.5290021896362305\n",
      "Epoch: 150, loss 0.5428555607795715\n",
      "Epoch: 150, loss 0.5286210179328918\n",
      "Epoch: 160, loss 0.31471386551856995\n",
      "Epoch: 160, loss 0.2542096674442291\n",
      "Epoch: 160, loss 0.250242680311203\n",
      "Epoch: 160, loss 0.5335372090339661\n",
      "Epoch: 160, loss 0.5163465142250061\n",
      "Epoch: 160, loss 0.5304576754570007\n",
      "Epoch: 160, loss 0.5396574139595032\n",
      "Epoch: 160, loss 0.528307318687439\n",
      "Epoch: 170, loss 0.29350578784942627\n",
      "Epoch: 170, loss 0.24708889424800873\n",
      "Epoch: 170, loss 0.24712298810482025\n",
      "Epoch: 170, loss 0.5356999039649963\n",
      "Epoch: 170, loss 0.5217245817184448\n",
      "Epoch: 170, loss 0.5315284132957458\n",
      "Epoch: 170, loss 0.5363062024116516\n",
      "Epoch: 170, loss 0.5275909304618835\n",
      "Epoch: 180, loss 0.2735154926776886\n",
      "Epoch: 180, loss 0.24080096185207367\n",
      "Epoch: 180, loss 0.24480028450489044\n",
      "Epoch: 180, loss 0.537502110004425\n",
      "Epoch: 180, loss 0.5275468230247498\n",
      "Epoch: 180, loss 0.5321915149688721\n",
      "Epoch: 180, loss 0.5329280495643616\n",
      "Epoch: 180, loss 0.5265301465988159\n",
      "Epoch: 190, loss 0.2550092041492462\n",
      "Epoch: 190, loss 0.2353486269712448\n",
      "Epoch: 190, loss 0.24321874976158142\n",
      "Epoch: 190, loss 0.5389306545257568\n",
      "Epoch: 190, loss 0.5336080193519592\n",
      "Epoch: 190, loss 0.5324687957763672\n",
      "Epoch: 190, loss 0.5296443104743958\n",
      "Epoch: 190, loss 0.525213897228241\n",
      "Epoch: 200, loss 0.23811808228492737\n",
      "Epoch: 200, loss 0.23068276047706604\n",
      "Epoch: 200, loss 0.2422843724489212\n",
      "Epoch: 200, loss 0.5400047302246094\n",
      "Epoch: 200, loss 0.5397099852561951\n",
      "Epoch: 200, loss 0.5324130058288574\n",
      "Epoch: 200, loss 0.5265516638755798\n",
      "Epoch: 200, loss 0.5237405300140381\n",
      "Epoch: 210, loss 0.22285889089107513\n",
      "Epoch: 210, loss 0.22672395408153534\n",
      "Epoch: 210, loss 0.2418869137763977\n",
      "Epoch: 210, loss 0.5407640933990479\n",
      "Epoch: 210, loss 0.5456888675689697\n",
      "Epoch: 210, loss 0.5320914387702942\n",
      "Epoch: 210, loss 0.5237129926681519\n",
      "Epoch: 210, loss 0.5221991539001465\n",
      "Epoch: 220, loss 0.20916733145713806\n",
      "Epoch: 220, loss 0.22338102757930756\n",
      "Epoch: 220, loss 0.24191750586032867\n",
      "Epoch: 220, loss 0.5412574410438538\n",
      "Epoch: 220, loss 0.5514257550239563\n",
      "Epoch: 220, loss 0.5315719246864319\n",
      "Epoch: 220, loss 0.5211583971977234\n",
      "Epoch: 220, loss 0.5206605792045593\n",
      "Epoch: 230, loss 0.1969311386346817\n",
      "Epoch: 230, loss 0.22056357562541962\n",
      "Epoch: 230, loss 0.2422782927751541\n",
      "Epoch: 230, loss 0.5415341258049011\n",
      "Epoch: 230, loss 0.5568445324897766\n",
      "Epoch: 230, loss 0.5309149026870728\n",
      "Epoch: 230, loss 0.5188921689987183\n",
      "Epoch: 230, loss 0.5191744565963745\n",
      "Epoch: 240, loss 0.18601542711257935\n",
      "Epoch: 240, loss 0.21818894147872925\n",
      "Epoch: 240, loss 0.24288658797740936\n",
      "Epoch: 240, loss 0.5416390299797058\n",
      "Epoch: 240, loss 0.5619040131568909\n",
      "Epoch: 240, loss 0.5301699638366699\n",
      "Epoch: 240, loss 0.5169013738632202\n",
      "Epoch: 240, loss 0.5177720189094543\n",
      "Epoch: 250, loss 0.17627973854541779\n",
      "Epoch: 250, loss 0.216185063123703\n",
      "Epoch: 250, loss 0.2436755895614624\n",
      "Epoch: 250, loss 0.541610836982727\n",
      "Epoch: 250, loss 0.5665889382362366\n",
      "Epoch: 250, loss 0.5293753147125244\n",
      "Epoch: 250, loss 0.5151631236076355\n",
      "Epoch: 250, loss 0.5164698362350464\n",
      "Epoch: 260, loss 0.1675885170698166\n",
      "Epoch: 260, loss 0.2144903689622879\n",
      "Epoch: 260, loss 0.2445921003818512\n",
      "Epoch: 260, loss 0.5414814949035645\n",
      "Epoch: 260, loss 0.5709018707275391\n",
      "Epoch: 260, loss 0.5285597443580627\n",
      "Epoch: 260, loss 0.51365065574646\n",
      "Epoch: 260, loss 0.5152742266654968\n",
      "Epoch: 270, loss 0.15981616079807281\n",
      "Epoch: 270, loss 0.21305344998836517\n",
      "Epoch: 270, loss 0.24559476971626282\n",
      "Epoch: 270, loss 0.541276752948761\n",
      "Epoch: 270, loss 0.5748565793037415\n",
      "Epoch: 270, loss 0.5277436375617981\n",
      "Epoch: 270, loss 0.512336254119873\n",
      "Epoch: 270, loss 0.5141847133636475\n",
      "Epoch: 280, loss 0.15284980833530426\n",
      "Epoch: 280, loss 0.21183161437511444\n",
      "Epoch: 280, loss 0.2466518133878708\n",
      "Epoch: 280, loss 0.5410171747207642\n",
      "Epoch: 280, loss 0.5784735679626465\n",
      "Epoch: 280, loss 0.5269412994384766\n",
      "Epoch: 280, loss 0.5111936926841736\n",
      "Epoch: 280, loss 0.5131967067718506\n",
      "Epoch: 290, loss 0.14658966660499573\n",
      "Epoch: 290, loss 0.21078942716121674\n",
      "Epoch: 290, loss 0.24773912131786346\n",
      "Epoch: 290, loss 0.5407187342643738\n",
      "Epoch: 290, loss 0.58177649974823\n",
      "Epoch: 290, loss 0.5261622071266174\n",
      "Epoch: 290, loss 0.5101991295814514\n",
      "Epoch: 290, loss 0.5123034715652466\n",
      "Epoch: 300, loss 0.1409483551979065\n",
      "Epoch: 300, loss 0.20989766716957092\n",
      "Epoch: 300, loss 0.2488381415605545\n",
      "Epoch: 300, loss 0.5403939485549927\n",
      "Epoch: 300, loss 0.5847903490066528\n",
      "Epoch: 300, loss 0.5254124402999878\n",
      "Epoch: 300, loss 0.5093316435813904\n",
      "Epoch: 300, loss 0.5114970803260803\n",
      "Epoch: 310, loss 0.13584989309310913\n",
      "Epoch: 310, loss 0.20913225412368774\n",
      "Epoch: 310, loss 0.24993519484996796\n",
      "Epoch: 310, loss 0.5400525331497192\n",
      "Epoch: 310, loss 0.5875400304794312\n",
      "Epoch: 310, loss 0.524695634841919\n",
      "Epoch: 310, loss 0.5085729360580444\n",
      "Epoch: 310, loss 0.5107695460319519\n",
      "Epoch: 320, loss 0.1312284767627716\n",
      "Epoch: 320, loss 0.20847304165363312\n",
      "Epoch: 320, loss 0.2510199248790741\n",
      "Epoch: 320, loss 0.5397018194198608\n",
      "Epoch: 320, loss 0.5900492072105408\n",
      "Epoch: 320, loss 0.5240134596824646\n",
      "Epoch: 320, loss 0.5079073309898376\n",
      "Epoch: 320, loss 0.5101128816604614\n",
      "Epoch: 330, loss 0.1270272433757782\n",
      "Epoch: 330, loss 0.2079034298658371\n",
      "Epoch: 330, loss 0.25208473205566406\n",
      "Epoch: 330, loss 0.5393475890159607\n",
      "Epoch: 330, loss 0.5923402309417725\n",
      "Epoch: 330, loss 0.5233665704727173\n",
      "Epoch: 330, loss 0.5073215365409851\n",
      "Epoch: 330, loss 0.5095197558403015\n",
      "Epoch: 340, loss 0.123197041451931\n",
      "Epoch: 340, loss 0.20740944147109985\n",
      "Epoch: 340, loss 0.2531241774559021\n",
      "Epoch: 340, loss 0.5389940738677979\n",
      "Epoch: 340, loss 0.5944337248802185\n",
      "Epoch: 340, loss 0.5227546691894531\n",
      "Epoch: 340, loss 0.506804347038269\n",
      "Epoch: 340, loss 0.5089834332466125\n",
      "Epoch: 350, loss 0.11969555914402008\n",
      "Epoch: 350, loss 0.20697972178459167\n",
      "Epoch: 350, loss 0.2541343569755554\n",
      "Epoch: 350, loss 0.5386446118354797\n",
      "Epoch: 350, loss 0.5963482856750488\n",
      "Epoch: 350, loss 0.5221768617630005\n",
      "Epoch: 350, loss 0.5063461661338806\n",
      "Epoch: 350, loss 0.5084976553916931\n",
      "Epoch: 360, loss 0.11648595333099365\n",
      "Epoch: 360, loss 0.20660440623760223\n",
      "Epoch: 360, loss 0.25511273741722107\n",
      "Epoch: 360, loss 0.5383015275001526\n",
      "Epoch: 360, loss 0.5981011390686035\n",
      "Epoch: 360, loss 0.5216318368911743\n",
      "Epoch: 360, loss 0.5059388279914856\n",
      "Epoch: 360, loss 0.5080569982528687\n",
      "Epoch: 370, loss 0.11353637278079987\n",
      "Epoch: 370, loss 0.2062753587961197\n",
      "Epoch: 370, loss 0.25605741143226624\n",
      "Epoch: 370, loss 0.5379665493965149\n",
      "Epoch: 370, loss 0.5997077226638794\n",
      "Epoch: 370, loss 0.5211182236671448\n",
      "Epoch: 370, loss 0.5055755972862244\n",
      "Epoch: 370, loss 0.507656455039978\n",
      "Epoch: 380, loss 0.11081910878419876\n",
      "Epoch: 380, loss 0.20598579943180084\n",
      "Epoch: 380, loss 0.25696757435798645\n",
      "Epoch: 380, loss 0.5376409292221069\n",
      "Epoch: 380, loss 0.6011817455291748\n",
      "Epoch: 380, loss 0.5206342935562134\n",
      "Epoch: 380, loss 0.5052506327629089\n",
      "Epoch: 380, loss 0.507291853427887\n",
      "Epoch: 390, loss 0.10830987989902496\n",
      "Epoch: 390, loss 0.20572978258132935\n",
      "Epoch: 390, loss 0.2578427493572235\n",
      "Epoch: 390, loss 0.5373256206512451\n",
      "Epoch: 390, loss 0.6025357842445374\n",
      "Epoch: 390, loss 0.520178496837616\n",
      "Epoch: 390, loss 0.5049589276313782\n",
      "Epoch: 390, loss 0.5069592595100403\n",
      "Epoch: 400, loss 0.10598772764205933\n",
      "Epoch: 400, loss 0.2055024653673172\n",
      "Epoch: 400, loss 0.25868281722068787\n",
      "Epoch: 400, loss 0.5370209813117981\n",
      "Epoch: 400, loss 0.6037811040878296\n",
      "Epoch: 400, loss 0.5197492241859436\n",
      "Epoch: 400, loss 0.5046964287757874\n",
      "Epoch: 400, loss 0.506655216217041\n",
      "Epoch: 410, loss 0.10383409261703491\n",
      "Epoch: 410, loss 0.20529964566230774\n",
      "Epoch: 410, loss 0.25948822498321533\n",
      "Epoch: 410, loss 0.5367275476455688\n",
      "Epoch: 410, loss 0.6049275398254395\n",
      "Epoch: 410, loss 0.5193448662757874\n",
      "Epoch: 410, loss 0.5044593811035156\n",
      "Epoch: 410, loss 0.5063768625259399\n",
      "Epoch: 420, loss 0.10183272510766983\n",
      "Epoch: 420, loss 0.20511768758296967\n",
      "Epoch: 420, loss 0.2602594494819641\n",
      "Epoch: 420, loss 0.5364453196525574\n",
      "Epoch: 420, loss 0.6059840321540833\n",
      "Epoch: 420, loss 0.5189639329910278\n",
      "Epoch: 420, loss 0.504244863986969\n",
      "Epoch: 420, loss 0.5061214566230774\n",
      "Epoch: 430, loss 0.09996914863586426\n",
      "Epoch: 430, loss 0.20495352149009705\n",
      "Epoch: 430, loss 0.26099714636802673\n",
      "Epoch: 430, loss 0.5361744165420532\n",
      "Epoch: 430, loss 0.6069589257240295\n",
      "Epoch: 430, loss 0.5186048746109009\n",
      "Epoch: 430, loss 0.504050076007843\n",
      "Epoch: 430, loss 0.5058867931365967\n",
      "Epoch: 440, loss 0.09823085367679596\n",
      "Epoch: 440, loss 0.20480471849441528\n",
      "Epoch: 440, loss 0.2617022395133972\n",
      "Epoch: 440, loss 0.5359146595001221\n",
      "Epoch: 440, loss 0.6078592538833618\n",
      "Epoch: 440, loss 0.5182662606239319\n",
      "Epoch: 440, loss 0.5038728713989258\n",
      "Epoch: 440, loss 0.5056706666946411\n",
      "Epoch: 450, loss 0.09660658240318298\n",
      "Epoch: 450, loss 0.20466884970664978\n",
      "Epoch: 450, loss 0.2623756527900696\n",
      "Epoch: 450, loss 0.53566575050354\n",
      "Epoch: 450, loss 0.6086913347244263\n",
      "Epoch: 450, loss 0.5179468989372253\n",
      "Epoch: 450, loss 0.5037111639976501\n",
      "Epoch: 450, loss 0.5054712891578674\n",
      "Epoch: 460, loss 0.09508629888296127\n",
      "Epoch: 460, loss 0.2045440822839737\n",
      "Epoch: 460, loss 0.2630184292793274\n",
      "Epoch: 460, loss 0.5354276299476624\n",
      "Epoch: 460, loss 0.6094613075256348\n",
      "Epoch: 460, loss 0.5176454782485962\n",
      "Epoch: 460, loss 0.5035633444786072\n",
      "Epoch: 460, loss 0.5052871108055115\n",
      "Epoch: 470, loss 0.09366099536418915\n",
      "Epoch: 470, loss 0.2044285386800766\n",
      "Epoch: 470, loss 0.26363128423690796\n",
      "Epoch: 470, loss 0.5352000594139099\n",
      "Epoch: 470, loss 0.6101745963096619\n",
      "Epoch: 470, loss 0.5173608660697937\n",
      "Epoch: 470, loss 0.5034279227256775\n",
      "Epoch: 470, loss 0.5051167011260986\n",
      "Epoch: 480, loss 0.09232289344072342\n",
      "Epoch: 480, loss 0.20432108640670776\n",
      "Epoch: 480, loss 0.2642155587673187\n",
      "Epoch: 480, loss 0.5349825620651245\n",
      "Epoch: 480, loss 0.6108355522155762\n",
      "Epoch: 480, loss 0.5170919895172119\n",
      "Epoch: 480, loss 0.5033035278320312\n",
      "Epoch: 480, loss 0.5049586892127991\n",
      "Epoch: 490, loss 0.09106471389532089\n",
      "Epoch: 490, loss 0.20422013103961945\n",
      "Epoch: 490, loss 0.264771968126297\n",
      "Epoch: 490, loss 0.5347749590873718\n",
      "Epoch: 490, loss 0.6114487648010254\n",
      "Epoch: 490, loss 0.5168378949165344\n",
      "Epoch: 490, loss 0.5031890869140625\n",
      "Epoch: 490, loss 0.5048120617866516\n",
      "Epoch: 500, loss 0.08988033980131149\n",
      "Epoch: 500, loss 0.2041250616312027\n",
      "Epoch: 500, loss 0.2653019428253174\n",
      "Epoch: 500, loss 0.5345768332481384\n",
      "Epoch: 500, loss 0.612017810344696\n",
      "Epoch: 500, loss 0.5165975093841553\n",
      "Epoch: 500, loss 0.5030835866928101\n",
      "Epoch: 500, loss 0.5046757459640503\n",
      "Epoch: 510, loss 0.0887637585401535\n",
      "Epoch: 510, loss 0.20403458178043365\n",
      "Epoch: 510, loss 0.2658061981201172\n",
      "Epoch: 510, loss 0.5343879461288452\n",
      "Epoch: 510, loss 0.612546443939209\n",
      "Epoch: 510, loss 0.5163701772689819\n",
      "Epoch: 510, loss 0.5029861927032471\n",
      "Epoch: 510, loss 0.5045487880706787\n",
      "Epoch: 520, loss 0.08770982176065445\n",
      "Epoch: 520, loss 0.20394788682460785\n",
      "Epoch: 520, loss 0.2662856876850128\n",
      "Epoch: 520, loss 0.534207820892334\n",
      "Epoch: 520, loss 0.6130378246307373\n",
      "Epoch: 520, loss 0.5161550045013428\n",
      "Epoch: 520, loss 0.5028960108757019\n",
      "Epoch: 520, loss 0.5044304728507996\n",
      "Epoch: 530, loss 0.08671397715806961\n",
      "Epoch: 530, loss 0.20386457443237305\n",
      "Epoch: 530, loss 0.26674169301986694\n",
      "Epoch: 530, loss 0.5340361595153809\n",
      "Epoch: 530, loss 0.6134945154190063\n",
      "Epoch: 530, loss 0.5159510970115662\n",
      "Epoch: 530, loss 0.5028125643730164\n",
      "Epoch: 530, loss 0.5043200850486755\n",
      "Epoch: 540, loss 0.08577185869216919\n",
      "Epoch: 540, loss 0.2037838101387024\n",
      "Epoch: 540, loss 0.26717495918273926\n",
      "Epoch: 540, loss 0.5338726043701172\n",
      "Epoch: 540, loss 0.6139193177223206\n",
      "Epoch: 540, loss 0.5157578587532043\n",
      "Epoch: 540, loss 0.5027350187301636\n",
      "Epoch: 540, loss 0.5042168498039246\n",
      "Epoch: 550, loss 0.08487963676452637\n",
      "Epoch: 550, loss 0.20370519161224365\n",
      "Epoch: 550, loss 0.26758643984794617\n",
      "Epoch: 550, loss 0.5337168574333191\n",
      "Epoch: 550, loss 0.6143146753311157\n",
      "Epoch: 550, loss 0.5155747532844543\n",
      "Epoch: 550, loss 0.5026629567146301\n",
      "Epoch: 550, loss 0.5041202902793884\n",
      "Epoch: 560, loss 0.0840337947010994\n",
      "Epoch: 560, loss 0.20362819731235504\n",
      "Epoch: 560, loss 0.2679769694805145\n",
      "Epoch: 560, loss 0.5335686206817627\n",
      "Epoch: 560, loss 0.6146828532218933\n",
      "Epoch: 560, loss 0.5154010057449341\n",
      "Epoch: 560, loss 0.5025960206985474\n",
      "Epoch: 560, loss 0.5040298104286194\n",
      "Epoch: 570, loss 0.0832311362028122\n",
      "Epoch: 570, loss 0.20355241000652313\n",
      "Epoch: 570, loss 0.26834744215011597\n",
      "Epoch: 570, loss 0.5334275960922241\n",
      "Epoch: 570, loss 0.6150255799293518\n",
      "Epoch: 570, loss 0.5152361392974854\n",
      "Epoch: 570, loss 0.502533495426178\n",
      "Epoch: 570, loss 0.5039449334144592\n",
      "Epoch: 580, loss 0.08246854692697525\n",
      "Epoch: 580, loss 0.20347747206687927\n",
      "Epoch: 580, loss 0.268698513507843\n",
      "Epoch: 580, loss 0.5332934260368347\n",
      "Epoch: 580, loss 0.6153450012207031\n",
      "Epoch: 580, loss 0.51507967710495\n",
      "Epoch: 580, loss 0.5024752020835876\n",
      "Epoch: 580, loss 0.5038653016090393\n",
      "Epoch: 590, loss 0.08174360543489456\n",
      "Epoch: 590, loss 0.20340323448181152\n",
      "Epoch: 590, loss 0.2690313160419464\n",
      "Epoch: 590, loss 0.5331658720970154\n",
      "Epoch: 590, loss 0.6156424880027771\n",
      "Epoch: 590, loss 0.5149310231208801\n",
      "Epoch: 590, loss 0.5024207234382629\n",
      "Epoch: 590, loss 0.5037904381752014\n",
      "Epoch: 600, loss 0.08105381578207016\n",
      "Epoch: 600, loss 0.20332935452461243\n",
      "Epoch: 600, loss 0.26934629678726196\n",
      "Epoch: 600, loss 0.5330445766448975\n",
      "Epoch: 600, loss 0.6159196496009827\n",
      "Epoch: 600, loss 0.5147898197174072\n",
      "Epoch: 600, loss 0.5023698210716248\n",
      "Epoch: 600, loss 0.5037200450897217\n",
      "Epoch: 610, loss 0.08039682358503342\n",
      "Epoch: 610, loss 0.2032555192708969\n",
      "Epoch: 610, loss 0.2696443796157837\n",
      "Epoch: 610, loss 0.5329293608665466\n",
      "Epoch: 610, loss 0.6161777973175049\n",
      "Epoch: 610, loss 0.514655590057373\n",
      "Epoch: 610, loss 0.5023221373558044\n",
      "Epoch: 610, loss 0.5036537647247314\n",
      "Epoch: 620, loss 0.07977061718702316\n",
      "Epoch: 620, loss 0.2031816691160202\n",
      "Epoch: 620, loss 0.2699261009693146\n",
      "Epoch: 620, loss 0.532819926738739\n",
      "Epoch: 620, loss 0.6164183020591736\n",
      "Epoch: 620, loss 0.5145279169082642\n",
      "Epoch: 620, loss 0.5022774338722229\n",
      "Epoch: 620, loss 0.5035912990570068\n",
      "Epoch: 630, loss 0.07917328923940659\n",
      "Epoch: 630, loss 0.20310769975185394\n",
      "Epoch: 630, loss 0.27019229531288147\n",
      "Epoch: 630, loss 0.5327160358428955\n",
      "Epoch: 630, loss 0.6166422963142395\n",
      "Epoch: 630, loss 0.5144065022468567\n",
      "Epoch: 630, loss 0.502235472202301\n",
      "Epoch: 630, loss 0.503532350063324\n",
      "Epoch: 640, loss 0.07860304415225983\n",
      "Epoch: 640, loss 0.20303332805633545\n",
      "Epoch: 640, loss 0.27044349908828735\n",
      "Epoch: 640, loss 0.532617449760437\n",
      "Epoch: 640, loss 0.6168509125709534\n",
      "Epoch: 640, loss 0.514290988445282\n",
      "Epoch: 640, loss 0.5021961331367493\n",
      "Epoch: 640, loss 0.5034767389297485\n",
      "Epoch: 650, loss 0.0780581533908844\n",
      "Epoch: 650, loss 0.20295840501785278\n",
      "Epoch: 650, loss 0.27068036794662476\n",
      "Epoch: 650, loss 0.5325239300727844\n",
      "Epoch: 650, loss 0.6170452237129211\n",
      "Epoch: 650, loss 0.5141810178756714\n",
      "Epoch: 650, loss 0.5021591186523438\n",
      "Epoch: 650, loss 0.5034241676330566\n",
      "Epoch: 660, loss 0.07753738760948181\n",
      "Epoch: 660, loss 0.20288307964801788\n",
      "Epoch: 660, loss 0.27090346813201904\n",
      "Epoch: 660, loss 0.5324351787567139\n",
      "Epoch: 660, loss 0.6172258853912354\n",
      "Epoch: 660, loss 0.514076292514801\n",
      "Epoch: 660, loss 0.5021242499351501\n",
      "Epoch: 660, loss 0.5033745169639587\n",
      "Epoch: 670, loss 0.07703909277915955\n",
      "Epoch: 670, loss 0.2028069645166397\n",
      "Epoch: 670, loss 0.2711133360862732\n",
      "Epoch: 670, loss 0.5323511362075806\n",
      "Epoch: 670, loss 0.6173940300941467\n",
      "Epoch: 670, loss 0.5139765739440918\n",
      "Epoch: 670, loss 0.5020915269851685\n",
      "Epoch: 670, loss 0.5033276081085205\n",
      "Epoch: 680, loss 0.07656218111515045\n",
      "Epoch: 680, loss 0.2027301788330078\n",
      "Epoch: 680, loss 0.2713105380535126\n",
      "Epoch: 680, loss 0.5322716236114502\n",
      "Epoch: 680, loss 0.6175501942634583\n",
      "Epoch: 680, loss 0.5138815641403198\n",
      "Epoch: 680, loss 0.5020606517791748\n",
      "Epoch: 680, loss 0.5032831430435181\n",
      "Epoch: 690, loss 0.07610532641410828\n",
      "Epoch: 690, loss 0.2026526778936386\n",
      "Epoch: 690, loss 0.2714956998825073\n",
      "Epoch: 690, loss 0.5321963429450989\n",
      "Epoch: 690, loss 0.6176952123641968\n",
      "Epoch: 690, loss 0.513791024684906\n",
      "Epoch: 690, loss 0.5020315647125244\n",
      "Epoch: 690, loss 0.5032409429550171\n",
      "Epoch: 700, loss 0.0756673589348793\n",
      "Epoch: 700, loss 0.20257431268692017\n",
      "Epoch: 700, loss 0.2716691195964813\n",
      "Epoch: 700, loss 0.5321250557899475\n",
      "Epoch: 700, loss 0.6178299188613892\n",
      "Epoch: 700, loss 0.5137047171592712\n",
      "Epoch: 700, loss 0.502004086971283\n",
      "Epoch: 700, loss 0.5032010674476624\n",
      "Epoch: 710, loss 0.0752473920583725\n",
      "Epoch: 710, loss 0.20249514281749725\n",
      "Epoch: 710, loss 0.2718314826488495\n",
      "Epoch: 710, loss 0.5320576429367065\n",
      "Epoch: 710, loss 0.6179544925689697\n",
      "Epoch: 710, loss 0.5136224627494812\n",
      "Epoch: 710, loss 0.5019782185554504\n",
      "Epoch: 710, loss 0.50316321849823\n",
      "Epoch: 720, loss 0.07484428584575653\n",
      "Epoch: 720, loss 0.2024150788784027\n",
      "Epoch: 720, loss 0.2719830274581909\n",
      "Epoch: 720, loss 0.531994104385376\n",
      "Epoch: 720, loss 0.6180700063705444\n",
      "Epoch: 720, loss 0.513543963432312\n",
      "Epoch: 720, loss 0.5019537210464478\n",
      "Epoch: 720, loss 0.5031272172927856\n",
      "Epoch: 730, loss 0.07445730268955231\n",
      "Epoch: 730, loss 0.20233409106731415\n",
      "Epoch: 730, loss 0.27212437987327576\n",
      "Epoch: 730, loss 0.5319340229034424\n",
      "Epoch: 730, loss 0.6181768178939819\n",
      "Epoch: 730, loss 0.5134691596031189\n",
      "Epoch: 730, loss 0.5019306540489197\n",
      "Epoch: 730, loss 0.5030931234359741\n",
      "Epoch: 740, loss 0.07408548891544342\n",
      "Epoch: 740, loss 0.20225214958190918\n",
      "Epoch: 740, loss 0.2722557783126831\n",
      "Epoch: 740, loss 0.5318774580955505\n",
      "Epoch: 740, loss 0.6182754635810852\n",
      "Epoch: 740, loss 0.513397753238678\n",
      "Epoch: 740, loss 0.5019088387489319\n",
      "Epoch: 740, loss 0.5030607581138611\n",
      "Epoch: 750, loss 0.07372807711362839\n",
      "Epoch: 750, loss 0.20216932892799377\n",
      "Epoch: 750, loss 0.27237772941589355\n",
      "Epoch: 750, loss 0.5318241715431213\n",
      "Epoch: 750, loss 0.6183661818504333\n",
      "Epoch: 750, loss 0.5133295655250549\n",
      "Epoch: 750, loss 0.5018881559371948\n",
      "Epoch: 750, loss 0.5030298829078674\n",
      "Epoch: 760, loss 0.07338432222604752\n",
      "Epoch: 760, loss 0.20208550989627838\n",
      "Epoch: 760, loss 0.27249059081077576\n",
      "Epoch: 760, loss 0.5317740440368652\n",
      "Epoch: 760, loss 0.6184498071670532\n",
      "Epoch: 760, loss 0.5132646560668945\n",
      "Epoch: 760, loss 0.5018686056137085\n",
      "Epoch: 760, loss 0.5030006170272827\n",
      "Epoch: 770, loss 0.07305356115102768\n",
      "Epoch: 770, loss 0.20200079679489136\n",
      "Epoch: 770, loss 0.27259472012519836\n",
      "Epoch: 770, loss 0.5317268967628479\n",
      "Epoch: 770, loss 0.6185263991355896\n",
      "Epoch: 770, loss 0.5132025480270386\n",
      "Epoch: 770, loss 0.5018501281738281\n",
      "Epoch: 770, loss 0.5029727220535278\n",
      "Epoch: 780, loss 0.07273515313863754\n",
      "Epoch: 780, loss 0.20191523432731628\n",
      "Epoch: 780, loss 0.2726905345916748\n",
      "Epoch: 780, loss 0.5316826701164246\n",
      "Epoch: 780, loss 0.6185964345932007\n",
      "Epoch: 780, loss 0.5131433606147766\n",
      "Epoch: 780, loss 0.5018326044082642\n",
      "Epoch: 780, loss 0.5029461979866028\n",
      "Epoch: 790, loss 0.07242836058139801\n",
      "Epoch: 790, loss 0.20182862877845764\n",
      "Epoch: 790, loss 0.272778183221817\n",
      "Epoch: 790, loss 0.5316411852836609\n",
      "Epoch: 790, loss 0.6186604499816895\n",
      "Epoch: 790, loss 0.5130867958068848\n",
      "Epoch: 790, loss 0.5018159747123718\n",
      "Epoch: 790, loss 0.5029209852218628\n",
      "Epoch: 800, loss 0.0721326544880867\n",
      "Epoch: 800, loss 0.2017410844564438\n",
      "Epoch: 800, loss 0.2728581428527832\n",
      "Epoch: 800, loss 0.5316023826599121\n",
      "Epoch: 800, loss 0.6187185645103455\n",
      "Epoch: 800, loss 0.5130327939987183\n",
      "Epoch: 800, loss 0.5018002390861511\n",
      "Epoch: 800, loss 0.5028969049453735\n",
      "Epoch: 810, loss 0.07184765487909317\n",
      "Epoch: 810, loss 0.20165280997753143\n",
      "Epoch: 810, loss 0.2729308307170868\n",
      "Epoch: 810, loss 0.5315660238265991\n",
      "Epoch: 810, loss 0.618770956993103\n",
      "Epoch: 810, loss 0.5129812955856323\n",
      "Epoch: 810, loss 0.5017853379249573\n",
      "Epoch: 810, loss 0.5028740167617798\n",
      "Epoch: 820, loss 0.07157260924577713\n",
      "Epoch: 820, loss 0.20156344771385193\n",
      "Epoch: 820, loss 0.2729962468147278\n",
      "Epoch: 820, loss 0.5315322279930115\n",
      "Epoch: 820, loss 0.6188184022903442\n",
      "Epoch: 820, loss 0.5129320621490479\n",
      "Epoch: 820, loss 0.5017712712287903\n",
      "Epoch: 820, loss 0.5028521418571472\n",
      "Epoch: 830, loss 0.0713072419166565\n",
      "Epoch: 830, loss 0.20147334039211273\n",
      "Epoch: 830, loss 0.2730550169944763\n",
      "Epoch: 830, loss 0.5315006971359253\n",
      "Epoch: 830, loss 0.6188606023788452\n",
      "Epoch: 830, loss 0.5128850340843201\n",
      "Epoch: 830, loss 0.5017578601837158\n",
      "Epoch: 830, loss 0.5028313398361206\n",
      "Epoch: 840, loss 0.07105103880167007\n",
      "Epoch: 840, loss 0.20138238370418549\n",
      "Epoch: 840, loss 0.27310711145401\n",
      "Epoch: 840, loss 0.5314714908599854\n",
      "Epoch: 840, loss 0.6188981533050537\n",
      "Epoch: 840, loss 0.5128400325775146\n",
      "Epoch: 840, loss 0.5017451643943787\n",
      "Epoch: 840, loss 0.5028114318847656\n",
      "Epoch: 850, loss 0.07080364227294922\n",
      "Epoch: 850, loss 0.20129063725471497\n",
      "Epoch: 850, loss 0.27315303683280945\n",
      "Epoch: 850, loss 0.5314443111419678\n",
      "Epoch: 850, loss 0.6189310550689697\n",
      "Epoch: 850, loss 0.5127971768379211\n",
      "Epoch: 850, loss 0.501733124256134\n",
      "Epoch: 850, loss 0.502792477607727\n",
      "Epoch: 860, loss 0.07056447118520737\n",
      "Epoch: 860, loss 0.20119792222976685\n",
      "Epoch: 860, loss 0.27319276332855225\n",
      "Epoch: 860, loss 0.5314192771911621\n",
      "Epoch: 860, loss 0.6189599633216858\n",
      "Epoch: 860, loss 0.5127561688423157\n",
      "Epoch: 860, loss 0.5017216801643372\n",
      "Epoch: 860, loss 0.5027743577957153\n",
      "Epoch: 870, loss 0.07033322006464005\n",
      "Epoch: 870, loss 0.20110441744327545\n",
      "Epoch: 870, loss 0.27322670817375183\n",
      "Epoch: 870, loss 0.531396210193634\n",
      "Epoch: 870, loss 0.6189848184585571\n",
      "Epoch: 870, loss 0.5127170085906982\n",
      "Epoch: 870, loss 0.501710832118988\n",
      "Epoch: 870, loss 0.5027570128440857\n",
      "Epoch: 880, loss 0.07010947167873383\n",
      "Epoch: 880, loss 0.20101013779640198\n",
      "Epoch: 880, loss 0.27325499057769775\n",
      "Epoch: 880, loss 0.5313751101493835\n",
      "Epoch: 880, loss 0.6190057992935181\n",
      "Epoch: 880, loss 0.5126796364784241\n",
      "Epoch: 880, loss 0.5017005801200867\n",
      "Epoch: 880, loss 0.5027405619621277\n",
      "Epoch: 890, loss 0.0698930099606514\n",
      "Epoch: 890, loss 0.20091502368450165\n",
      "Epoch: 890, loss 0.27327796816825867\n",
      "Epoch: 890, loss 0.5313557386398315\n",
      "Epoch: 890, loss 0.6190232634544373\n",
      "Epoch: 890, loss 0.5126439929008484\n",
      "Epoch: 890, loss 0.5016908049583435\n",
      "Epoch: 890, loss 0.5027247667312622\n",
      "Epoch: 900, loss 0.06968352943658829\n",
      "Epoch: 900, loss 0.2008192539215088\n",
      "Epoch: 900, loss 0.2732957899570465\n",
      "Epoch: 900, loss 0.5313381552696228\n",
      "Epoch: 900, loss 0.6190371513366699\n",
      "Epoch: 900, loss 0.5126098394393921\n",
      "Epoch: 900, loss 0.5016815662384033\n",
      "Epoch: 900, loss 0.5027097463607788\n",
      "Epoch: 910, loss 0.06948064267635345\n",
      "Epoch: 910, loss 0.20072275400161743\n",
      "Epoch: 910, loss 0.273308664560318\n",
      "Epoch: 910, loss 0.5313222408294678\n",
      "Epoch: 910, loss 0.6190477013587952\n",
      "Epoch: 910, loss 0.5125772953033447\n",
      "Epoch: 910, loss 0.5016728043556213\n",
      "Epoch: 910, loss 0.5026952624320984\n",
      "Epoch: 920, loss 0.06928408890962601\n",
      "Epoch: 920, loss 0.20062556862831116\n",
      "Epoch: 920, loss 0.27331677079200745\n",
      "Epoch: 920, loss 0.5313079357147217\n",
      "Epoch: 920, loss 0.6190549731254578\n",
      "Epoch: 920, loss 0.5125461220741272\n",
      "Epoch: 920, loss 0.501664400100708\n",
      "Epoch: 920, loss 0.5026815533638\n",
      "Epoch: 930, loss 0.06909345835447311\n",
      "Epoch: 930, loss 0.20052750408649445\n",
      "Epoch: 930, loss 0.27332016825675964\n",
      "Epoch: 930, loss 0.5312952399253845\n",
      "Epoch: 930, loss 0.6190596222877502\n",
      "Epoch: 930, loss 0.512516438961029\n",
      "Epoch: 930, loss 0.5016564726829529\n",
      "Epoch: 930, loss 0.5026683807373047\n",
      "Epoch: 940, loss 0.06890866160392761\n",
      "Epoch: 940, loss 0.20042894780635834\n",
      "Epoch: 940, loss 0.2733193337917328\n",
      "Epoch: 940, loss 0.5312838554382324\n",
      "Epoch: 940, loss 0.6190611720085144\n",
      "Epoch: 940, loss 0.5124880075454712\n",
      "Epoch: 940, loss 0.501649022102356\n",
      "Epoch: 940, loss 0.5026557445526123\n",
      "Epoch: 950, loss 0.0687294602394104\n",
      "Epoch: 950, loss 0.20032966136932373\n",
      "Epoch: 950, loss 0.2733142077922821\n",
      "Epoch: 950, loss 0.5312740206718445\n",
      "Epoch: 950, loss 0.6190599203109741\n",
      "Epoch: 950, loss 0.5124609470367432\n",
      "Epoch: 950, loss 0.5016419291496277\n",
      "Epoch: 950, loss 0.5026437640190125\n",
      "Epoch: 960, loss 0.06855561584234238\n",
      "Epoch: 960, loss 0.20022982358932495\n",
      "Epoch: 960, loss 0.27330508828163147\n",
      "Epoch: 960, loss 0.5312654972076416\n",
      "Epoch: 960, loss 0.619055986404419\n",
      "Epoch: 960, loss 0.5124350786209106\n",
      "Epoch: 960, loss 0.5016351938247681\n",
      "Epoch: 960, loss 0.502632200717926\n",
      "Epoch: 970, loss 0.06838679313659668\n",
      "Epoch: 970, loss 0.20012938976287842\n",
      "Epoch: 970, loss 0.27329206466674805\n",
      "Epoch: 970, loss 0.5312583446502686\n",
      "Epoch: 970, loss 0.6190497279167175\n",
      "Epoch: 970, loss 0.5124103426933289\n",
      "Epoch: 970, loss 0.5016288161277771\n",
      "Epoch: 970, loss 0.5026211738586426\n",
      "Epoch: 980, loss 0.06822283565998077\n",
      "Epoch: 980, loss 0.20002834498882294\n",
      "Epoch: 980, loss 0.2732751667499542\n",
      "Epoch: 980, loss 0.531252384185791\n",
      "Epoch: 980, loss 0.6190410852432251\n",
      "Epoch: 980, loss 0.5123867988586426\n",
      "Epoch: 980, loss 0.5016227960586548\n",
      "Epoch: 980, loss 0.5026106238365173\n",
      "Epoch: 990, loss 0.06806349009275436\n",
      "Epoch: 990, loss 0.19992667436599731\n",
      "Epoch: 990, loss 0.27325472235679626\n",
      "Epoch: 990, loss 0.5312476754188538\n",
      "Epoch: 990, loss 0.619030237197876\n",
      "Epoch: 990, loss 0.5123642683029175\n",
      "Epoch: 990, loss 0.5016170144081116\n",
      "Epoch: 990, loss 0.5026004910469055\n"
     ]
    }
   ],
   "source": [
    "def train(lr: float) -> None:\n",
    "    mean_square_error = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1000):\n",
    "        for x_batch, t_batch in zip(train_x_data, train_t_data_onehot):\n",
    "            x_batch = x_batch.type(torch.float)\n",
    "            t_batch = t_batch.type(torch.float)\n",
    "\n",
    "            y_data = model(x_batch)\n",
    "            loss = mean_square_error(y_data, t_batch)\n",
    "            \n",
    "            # backpropagation (Automatic Differentiation)\n",
    "            ...\n",
    "            ...\n",
    "            ...\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch: {epoch}, loss {loss}')\n",
    "\n",
    "\n",
    "train(lr=1e-3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-5. 모델 추론 (evaluate)\n",
    "\n",
    "파이토치 튜토리얼을 참고하여 추론 코드를 구현하시오. (구글링 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0]) tensor([[0]])\n"
     ]
    }
   ],
   "source": [
    "def test(model=model):\n",
    "    t_batch = test_x_data.type(torch.float)\n",
    "    y_data = ...\n",
    "    print(y_data.argmax(dim=1), test_t_data)\n",
    "    \n",
    "    \n",
    "test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 전이 학습 (Transfer Learning)\n",
    "\n",
    "- GPU가 없다면 Google Colab에서 실습 권장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 데이터 불러오기\n",
    "\n",
    "파이토치 튜토리얼을 활용하여 CIFAR-10 데이터셋을 불러오시오. (구글링 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n"
     ]
    }
   ],
   "source": [
    "training_data = ...\n",
    "test_data = ...\n",
    "\n",
    "train_dataloader = ...\n",
    "test_dataloader = ...\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f'Shape of X [batch size, image channels, image height, image width]: {X.shape}')\n",
    "    print(f'Shape of y: {y.shape} {y.dtype}')\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. 이미지 분류(Image Classification) CNN (Convolution Neural Network) 모델 ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import resnet50\n",
    "\n",
    "\n",
    "model = resnet50(num_classes=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 모델 학습 (train)\n",
    "\n",
    "파이토치 튜토리얼을 참고하여 GPU를 이용하는 학습 코드를 구현하시오. (구글링 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (192x2x2). Calculated output size: (192x0x0). Output size is too small",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m             current \u001b[39m=\u001b[39m batch \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(images)\n\u001b[1;32m     23\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m:\u001b[39;00m\u001b[39m>7f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m  [\u001b[39m\u001b[39m{\u001b[39;00mcurrent\u001b[39m:\u001b[39;00m\u001b[39m>5d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m:\u001b[39;00m\u001b[39m>5d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m train(lr\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[35], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(lr)\u001b[0m\n\u001b[1;32m     10\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\u001b[39m.\u001b[39mrepeat((\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m     11\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m preds \u001b[39m=\u001b[39m model(images)\n\u001b[1;32m     14\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(preds, targets)\n\u001b[1;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/miniconda3/envs/hnvlib/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/hnvlib/lib/python3.8/site-packages/torchvision/models/alexnet.py:48\u001b[0m, in \u001b[0;36mAlexNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 48\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures(x)\n\u001b[1;32m     49\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m     50\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hnvlib/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/hnvlib/lib/python3.8/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/hnvlib/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/hnvlib/lib/python3.8/site-packages/torch/nn/modules/pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor):\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    167\u001b[0m                         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, ceil_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mceil_mode,\n\u001b[1;32m    168\u001b[0m                         return_indices\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_indices)\n",
      "File \u001b[0;32m~/miniconda3/envs/hnvlib/lib/python3.8/site-packages/torch/_jit_internal.py:485\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m if_true(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    484\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 485\u001b[0m     \u001b[39mreturn\u001b[39;00m if_false(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/hnvlib/lib/python3.8/site-packages/torch/nn/functional.py:782\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[39mif\u001b[39;00m stride \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m     stride \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mannotate(List[\u001b[39mint\u001b[39m], [])\n\u001b[0;32m--> 782\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mmax_pool2d(\u001b[39minput\u001b[39;49m, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (192x2x2). Calculated output size: (192x0x0). Output size is too small"
     ]
    }
   ],
   "source": [
    "def train(lr: float) -> None:\n",
    "    loss_fn = ...\n",
    "    optimizer = ...\n",
    "    device = ...\n",
    "    model.to(device)\n",
    "\n",
    "    size = len(train_dataloader.dataset)\n",
    "    model.train()\n",
    "    for epoch in range(3):\n",
    "        print(f'[Epoch {epoch}]')\n",
    "        for batch, (images, targets) in enumerate(train_dataloader):\n",
    "            images = ...\n",
    "            targets = ...\n",
    "\n",
    "            preds = ...\n",
    "            loss = ...\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            ...\n",
    "            ...\n",
    "\n",
    "            if batch % 10 == 0:\n",
    "                loss = loss.item()\n",
    "                current = batch * len(images)\n",
    "                print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "        print()\n",
    "\n",
    "\n",
    "train(lr=1e-2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 모델 검증 (validate)\n",
    "\n",
    "파이토치 튜토리얼을 참고하여 GPU를 이용하는 검증 코드를 구현하시오. (구글링 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate() -> None:\n",
    "    loss_fn = ...\n",
    "    device = ...\n",
    "    model.to(device)\n",
    "\n",
    "    size = len(test_dataloader.dataset)\n",
    "    num_batches = len(test_dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_dataloader:\n",
    "            images = ...\n",
    "            targets = ...\n",
    "\n",
    "            preds = ...\n",
    "\n",
    "            test_loss += loss_fn(preds, targets).item()\n",
    "            correct += (preds.argmax(1) == targets).float().sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n')\n",
    "        \n",
    "\n",
    "validate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 전이 학습 (Transfer Learning)\n",
    "\n",
    "파이토치를 활용하여 GPU를 이용하는 전이 학습 코드를 구현하시오. (구글링 가능)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.resnet import ResNet50_Weights, Bottleneck\n",
    "\n",
    "\n",
    "trans_model = resnet50(...)\n",
    "num_classes = 10\n",
    "trans_model.fc = nn.Linear(..., num_classes)\n",
    "device = ...\n",
    "trans_model.to(device)\n",
    "\n",
    "loss_fn = ...\n",
    "lr = 1e-2\n",
    "optimizer = ...\n",
    "\n",
    "# train\n",
    "size = len(train_dataloader.dataset)\n",
    "trans_model.train()\n",
    "for epoch in range(3):\n",
    "    print(f'[Epoch {epoch}]')\n",
    "    for batch, (images, targets) in enumerate(train_dataloader):\n",
    "        images = ...\n",
    "        targets = ...\n",
    "\n",
    "        preds = ...\n",
    "        loss = ...\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        ...\n",
    "        ...\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss = loss.item()\n",
    "            current = batch * len(images)\n",
    "            print(f'loss: {loss:>7f}  [{current:>5d}/{size:>5d}]')\n",
    "    print()\n",
    "\n",
    "# validate\n",
    "size = len(test_dataloader.dataset)\n",
    "num_batches = len(test_dataloader)\n",
    "trans_model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for images, targets in test_dataloader:\n",
    "        images = ...\n",
    "        targets = ...\n",
    "\n",
    "        preds = ...\n",
    "\n",
    "        test_loss += loss_fn(preds, targets).item()\n",
    "        correct += (preds.argmax(1) == targets).float().sum().item()\n",
    "test_loss /= num_batches\n",
    "correct /= size\n",
    "print(f'Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hnvlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
