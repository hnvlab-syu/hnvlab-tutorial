{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implemented and written by Yeoreum Lee in AI HnV Lab @ Sahmyook University in 2023\n",
    "__author__ = 'leeyeoreum02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Callable\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 2) (9, 1)\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array([[2, 4], [4, 11], [6, 6], [8, 5], [10, 7], [12, 16], [14, 8], [16, 3], [18, 7]])\n",
    "t_data = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1]).reshape(9, 1)\n",
    "\n",
    "print(x_data.shape, t_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 나누기(split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 2) (8, 1) (1, 2) (1, 1)\n"
     ]
    }
   ],
   "source": [
    "def split_data(x_data: np.ndarray, t_data: np.ndarray, split_rate: float) -> Tuple[np.ndarray]:\n",
    "    test_x_data = x_data[:int(split_rate * len(x_data))]\n",
    "    test_t_data = t_data[:int(split_rate * len(t_data))]\n",
    "    train_x_data = x_data[int(split_rate * len(x_data)):]\n",
    "    train_t_data = t_data[int(split_rate * len(t_data)):]\n",
    "    \n",
    "    return train_x_data, train_t_data, test_x_data, test_t_data\n",
    "\n",
    "\n",
    "train_x_data, train_t_data, test_x_data, test_t_data = split_data(x_data, t_data, split_rate=0.2)\n",
    "print(train_x_data.shape, train_t_data.shape, test_x_data.shape, test_t_data.shape,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 활성 함수(activation function)\n",
    "\n",
    "주어진 수식만을 이용하여 활성 함수 중 하나인 sigmoid 함수를 구현하시오. (구글링, 메인 교재 참고, 서브 강의 참고 금지)\n",
    "\n",
    "$$sigmoid(\\boldsymbol{x}) = \\frac {1} {1 + e^{-\\boldsymbol{x}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    return ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 로지스틱 회귀(logistic regression) 모델\n",
    "\n",
    "메인 교재와 서브 강의만을 이용하여 로지스틱 회귀(logistic regression) 모델을 구현하시오. (구글링 금지)\n",
    "\n",
    "$$\\boldsymbol{y} = f(W, b)(\\boldsymbol{x}) = sigmoid(W\\boldsymbol{x} + b)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, n_input: int, n_output: int) -> None:\n",
    "        ...\n",
    "        \n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        ...\n",
    "        return y\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        return self.forward(x)\n",
    "    \n",
    "    \n",
    "model = LogisticRegression(n_input=..., n_output=...)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 오차 함수 (error function, loss function)\n",
    "\n",
    "주어진 수식만을 이용하여 CE(cross entropy) 오차 함수를 구현하시오. (구글링, 메인 교재 참고, 서브 강의 참고 금지)\n",
    "- delta는 log의 진수 조건을 만족하기 위해 필요하므로 반드시 사용\n",
    "- N은 데이터 개수 (행 개수)\n",
    "- $y$는 정답(label) $\\hat{y}$은 예측값(prediction)\n",
    "\n",
    "$$CE = -\\sum_{i=1} ^N (\\boldsymbol{y_{i}} \\cdot \\log \\boldsymbol{\\hat{y_{i}}} + (1 - \\boldsymbol{y_{i}}) \\cdot \\log (1 - \\boldsymbol{\\hat{y_{i}}}))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_data: np.ndarray, t_data: np.ndarray) -> np.ndarray:\n",
    "    delta = 1e-4\n",
    "    return ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 편미분 함수 (partial numerical derivative)\n",
    "\n",
    "메인 교재와 서브 강의만을을 이용하여 야코비안 행렬(jacobian matrix)을 반환하는 편미분(partial numerical derivative) 함수를 구현하시오. (구글링 금지)\n",
    "\n",
    "$$J(W) = \\frac{dE} {dW} = \\begin{pmatrix} \\frac{\\partial E} {\\partial W_{11}} & \\frac {\\partial E} {\\partial W_{12}} \\end{pmatrix}$$\n",
    "\n",
    "$$J(\\boldsymbol{b}) = \\frac{dE} {d\\boldsymbol{b}} = \\begin{pmatrix} \\frac{\\partial E} {\\partial b_{1}} \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f: Callable, x: np.ndarray) -> np.ndarray:\n",
    "    ...\n",
    "    return grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 모델 학습 (train)\n",
    "\n",
    "메인 교재와 서브 강의만을 이용하여 다음 순서를 가지는 학습 코드를 구현하시오. (구글링 금지)\n",
    "\n",
    "1. 모델 순전파 (forward)\n",
    "2. 오차 계산 (loss)\n",
    "3. 모델 파라미터(가중치 + 편향) 별 오차 함수의 편미분값 계산 (numerical derivative)\n",
    "4. 가중치(weight), 편향(bias) 갱신 (경사 하강법, gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, step: 0, loss 8.597628926925902\n",
      "Epoch: 1, step: 0, loss 6.951631715461357\n",
      "Epoch: 2, step: 0, loss 4.181021138861848\n",
      "Epoch: 3, step: 0, loss 2.458987870303496\n",
      "Epoch: 4, step: 0, loss 1.932399865540985\n",
      "Epoch: 5, step: 0, loss 1.6467775444387929\n",
      "Epoch: 6, step: 0, loss 1.4566426916673794\n",
      "Epoch: 7, step: 0, loss 1.3210407289362165\n",
      "Epoch: 8, step: 0, loss 1.2192286195633546\n",
      "Epoch: 9, step: 0, loss 1.139814108577824\n",
      "Epoch: 10, step: 0, loss 1.0760904277222223\n",
      "Epoch: 11, step: 0, loss 1.0238487440398594\n",
      "Epoch: 12, step: 0, loss 0.9803004610316339\n",
      "Epoch: 13, step: 0, loss 0.943512820273392\n",
      "Epoch: 14, step: 0, loss 0.9120955951421462\n",
      "Epoch: 15, step: 0, loss 0.8850177950329314\n",
      "Epoch: 16, step: 0, loss 0.8614953222098706\n",
      "Epoch: 17, step: 0, loss 0.8409192369431691\n",
      "Epoch: 18, step: 0, loss 0.8228082776247566\n",
      "Epoch: 19, step: 0, loss 0.8067764321922363\n",
      "Epoch: 20, step: 0, loss 0.7925101734669022\n",
      "Epoch: 21, step: 0, loss 0.7797520906538181\n",
      "Epoch: 22, step: 0, loss 0.7682888701017578\n",
      "Epoch: 23, step: 0, loss 0.7579423053004876\n",
      "Epoch: 24, step: 0, loss 0.7485624622279333\n",
      "Epoch: 25, step: 0, loss 0.7400224076559556\n",
      "Epoch: 26, step: 0, loss 0.7322140902260516\n",
      "Epoch: 27, step: 0, loss 0.7250450847587406\n",
      "Epoch: 28, step: 0, loss 0.718435991874725\n",
      "Epoch: 29, step: 0, loss 0.7123183412756728\n",
      "Epoch: 30, step: 0, loss 0.7066328864949272\n",
      "Epoch: 31, step: 0, loss 0.7013282070626295\n",
      "Epoch: 32, step: 0, loss 0.6963595543636952\n",
      "Epoch: 33, step: 0, loss 0.6916878923663957\n",
      "Epoch: 34, step: 0, loss 0.6872790954443069\n",
      "Epoch: 35, step: 0, loss 0.6831032737890212\n",
      "Epoch: 36, step: 0, loss 0.6791342031869197\n",
      "Epoch: 37, step: 0, loss 0.6753488407149948\n",
      "Epoch: 38, step: 0, loss 0.6717269116131412\n",
      "Epoch: 39, step: 0, loss 0.6682505554544881\n",
      "Epoch: 40, step: 0, loss 0.6649040219858917\n",
      "Epoch: 41, step: 0, loss 0.6616734087866133\n",
      "Epoch: 42, step: 0, loss 0.6585464342934583\n",
      "Epoch: 43, step: 0, loss 0.6555122408772349\n",
      "Epoch: 44, step: 0, loss 0.6525612235519518\n",
      "Epoch: 45, step: 0, loss 0.6496848806330265\n",
      "Epoch: 46, step: 0, loss 0.6468756832587534\n",
      "Epoch: 47, step: 0, loss 0.6441269611713732\n",
      "Epoch: 48, step: 0, loss 0.6414328025637366\n",
      "Epoch: 49, step: 0, loss 0.6387879661169591\n",
      "Epoch: 50, step: 0, loss 0.6361878036409238\n",
      "Epoch: 51, step: 0, loss 0.633628191951873\n",
      "Epoch: 52, step: 0, loss 0.6311054728163957\n",
      "Epoch: 53, step: 0, loss 0.6286163999534841\n",
      "Epoch: 54, step: 0, loss 0.6261580922226618\n",
      "Epoch: 55, step: 0, loss 0.6237279922431398\n",
      "Epoch: 56, step: 0, loss 0.6213238297868745\n",
      "Epoch: 57, step: 0, loss 0.6189435893745349\n",
      "Epoch: 58, step: 0, loss 0.6165854815758433\n",
      "Epoch: 59, step: 0, loss 0.6142479175783511\n",
      "Epoch: 60, step: 0, loss 0.6119294866418893\n",
      "Epoch: 61, step: 0, loss 0.6096289361042138\n",
      "Epoch: 62, step: 0, loss 0.607345153642178\n",
      "Epoch: 63, step: 0, loss 0.6050771515310294\n",
      "Epoch: 64, step: 0, loss 0.6028240526722952\n",
      "Epoch: 65, step: 0, loss 0.6005850781877826\n",
      "Epoch: 66, step: 0, loss 0.5983595364039308\n",
      "Epoch: 67, step: 0, loss 0.5961468130694915\n",
      "Epoch: 68, step: 0, loss 0.5939463626637665\n",
      "Epoch: 69, step: 0, loss 0.5917577006780934\n",
      "Epoch: 70, step: 0, loss 0.5895803967561557\n",
      "Epoch: 71, step: 0, loss 0.5874140686016146\n",
      "Epoch: 72, step: 0, loss 0.5852583765627185\n",
      "Epoch: 73, step: 0, loss 0.5831130188215456\n",
      "Epoch: 74, step: 0, loss 0.5809777271196921\n",
      "Epoch: 75, step: 0, loss 0.578852262959172\n",
      "Epoch: 76, step: 0, loss 0.5767364142277022\n",
      "Epoch: 77, step: 0, loss 0.574629992198571\n",
      "Epoch: 78, step: 0, loss 0.5725328288677919\n",
      "Epoch: 79, step: 0, loss 0.5704447745843267\n",
      "Epoch: 80, step: 0, loss 0.5683656959485226\n",
      "Epoch: 81, step: 0, loss 0.5662954739419759\n",
      "Epoch: 82, step: 0, loss 0.5642340022691285\n",
      "Epoch: 83, step: 0, loss 0.562181185881347\n",
      "Epoch: 84, step: 0, loss 0.5601369396682545\n",
      "Epoch: 85, step: 0, loss 0.5581011872946948\n",
      "Epoch: 86, step: 0, loss 0.5560738601681053\n",
      "Epoch: 87, step: 0, loss 0.5540548965213461\n",
      "Epoch: 88, step: 0, loss 0.552044240599022\n",
      "Epoch: 89, step: 0, loss 0.5500418419357553\n",
      "Epoch: 90, step: 0, loss 0.5480476547135805\n",
      "Epoch: 91, step: 0, loss 0.5460616371951591\n",
      "Epoch: 92, step: 0, loss 0.5440837512176362\n",
      "Epoch: 93, step: 0, loss 0.5421139617458237\n",
      "Epoch: 94, step: 0, loss 0.5401522364758949\n",
      "Epoch: 95, step: 0, loss 0.5381985454825666\n",
      "Epoch: 96, step: 0, loss 0.5362528609089862\n",
      "Epoch: 97, step: 0, loss 0.5343151566895492\n",
      "Epoch: 98, step: 0, loss 0.5323854083057452\n",
      "Epoch: 99, step: 0, loss 0.530463592570701\n",
      "Epoch: 100, step: 0, loss 0.5285496874363628\n",
      "Epoch: 101, step: 0, loss 0.5266436718255422\n",
      "Epoch: 102, step: 0, loss 0.5247455254819412\n",
      "Epoch: 103, step: 0, loss 0.5228552288378367\n",
      "Epoch: 104, step: 0, loss 0.520972762898105\n",
      "Epoch: 105, step: 0, loss 0.5190981091373595\n",
      "Epoch: 106, step: 0, loss 0.5172312494088266\n",
      "Epoch: 107, step: 0, loss 0.5153721658657929\n",
      "Epoch: 108, step: 0, loss 0.5135208408906068\n",
      "Epoch: 109, step: 0, loss 0.5116772570339222\n",
      "Epoch: 110, step: 0, loss 0.5098413969607806\n",
      "Epoch: 111, step: 0, loss 0.5080132434030844\n",
      "Epoch: 112, step: 0, loss 0.5061927791191412\n",
      "Epoch: 113, step: 0, loss 0.5043799868571655\n",
      "Epoch: 114, step: 0, loss 0.5025748493250023\n",
      "Epoch: 115, step: 0, loss 0.5007773491624626\n",
      "Epoch: 116, step: 0, loss 0.49898746891827056\n",
      "Epoch: 117, step: 0, loss 0.49720519103054317\n",
      "Epoch: 118, step: 0, loss 0.49543049780918375\n",
      "Epoch: 119, step: 0, loss 0.49366337142199235\n",
      "Epoch: 120, step: 0, loss 0.49190379388154587\n",
      "Epoch: 121, step: 0, loss 0.4901517470362762\n",
      "Epoch: 122, step: 0, loss 0.4884072125608425\n",
      "Epoch: 123, step: 0, loss 0.48667017195021045\n",
      "Epoch: 124, step: 0, loss 0.48494060651353993\n",
      "Epoch: 125, step: 0, loss 0.48321849737072814\n",
      "Epoch: 126, step: 0, loss 0.4815038254486453\n",
      "Epoch: 127, step: 0, loss 0.47979657147943866\n",
      "Epoch: 128, step: 0, loss 0.4780967159986913\n",
      "Epoch: 129, step: 0, loss 0.47640423934547715\n",
      "Epoch: 130, step: 0, loss 0.47471912166203717\n",
      "Epoch: 131, step: 0, loss 0.47304134289455757\n",
      "Epoch: 132, step: 0, loss 0.4713708827942476\n",
      "Epoch: 133, step: 0, loss 0.46970772091944285\n",
      "Epoch: 134, step: 0, loss 0.4680518366366016\n",
      "Epoch: 135, step: 0, loss 0.46640320912349675\n",
      "Epoch: 136, step: 0, loss 0.4647618173714434\n",
      "Epoch: 137, step: 0, loss 0.4631276401887062\n",
      "Epoch: 138, step: 0, loss 0.4615006562022721\n",
      "Epoch: 139, step: 0, loss 0.4598808438625048\n",
      "Epoch: 140, step: 0, loss 0.458268181446401\n",
      "Epoch: 141, step: 0, loss 0.45666264705989007\n",
      "Epoch: 142, step: 0, loss 0.4550642186429613\n",
      "Epoch: 143, step: 0, loss 0.45347287397287844\n",
      "Epoch: 144, step: 0, loss 0.4518885906678042\n",
      "Epoch: 145, step: 0, loss 0.45031134619103963\n",
      "Epoch: 146, step: 0, loss 0.4487411178547508\n",
      "Epoch: 147, step: 0, loss 0.44717788282432946\n",
      "Epoch: 148, step: 0, loss 0.4456216181218669\n",
      "Epoch: 149, step: 0, loss 0.44407230063049397\n",
      "Epoch: 150, step: 0, loss 0.4425299070984636\n",
      "Epoch: 151, step: 0, loss 0.44099441414336715\n",
      "Epoch: 152, step: 0, loss 0.43946579825540844\n",
      "Epoch: 153, step: 0, loss 0.43794403580252467\n",
      "Epoch: 154, step: 0, loss 0.4364291030334728\n",
      "Epoch: 155, step: 0, loss 0.43492097608232\n",
      "Epoch: 156, step: 0, loss 0.4334196309725849\n",
      "Epoch: 157, step: 0, loss 0.43192504362042994\n",
      "Epoch: 158, step: 0, loss 0.4304371898394973\n",
      "Epoch: 159, step: 0, loss 0.4289560453439332\n",
      "Epoch: 160, step: 0, loss 0.4274815857530339\n",
      "Epoch: 161, step: 0, loss 0.4260137865943618\n",
      "Epoch: 162, step: 0, loss 0.42455262330817367\n",
      "Epoch: 163, step: 0, loss 0.42309807125068566\n",
      "Epoch: 164, step: 0, loss 0.42165010569772204\n",
      "Epoch: 165, step: 0, loss 0.4202087018487483\n",
      "Epoch: 166, step: 0, loss 0.41877383483017316\n",
      "Epoch: 167, step: 0, loss 0.41734547969916713\n",
      "Epoch: 168, step: 0, loss 0.41592361144698564\n",
      "Epoch: 169, step: 0, loss 0.4145082050023003\n",
      "Epoch: 170, step: 0, loss 0.41309923523510533\n",
      "Epoch: 171, step: 0, loss 0.41169667695947415\n",
      "Epoch: 172, step: 0, loss 0.41030050493757053\n",
      "Epoch: 173, step: 0, loss 0.4089106938822372\n",
      "Epoch: 174, step: 0, loss 0.4075272184607642\n",
      "Epoch: 175, step: 0, loss 0.4061500532977805\n",
      "Epoch: 176, step: 0, loss 0.4047791729788111\n",
      "Epoch: 177, step: 0, loss 0.4034145520526342\n",
      "Epoch: 178, step: 0, loss 0.4020561650348829\n",
      "Epoch: 179, step: 0, loss 0.4007039864112326\n",
      "Epoch: 180, step: 0, loss 0.39935799063949884\n",
      "Epoch: 181, step: 0, loss 0.3980181521536418\n",
      "Epoch: 182, step: 0, loss 0.3966844453657939\n",
      "Epoch: 183, step: 0, loss 0.39535684466924\n",
      "Epoch: 184, step: 0, loss 0.3940353244414079\n",
      "Epoch: 185, step: 0, loss 0.3927198590464936\n",
      "Epoch: 186, step: 0, loss 0.39141042283809735\n",
      "Epoch: 187, step: 0, loss 0.3901069901617781\n",
      "Epoch: 188, step: 0, loss 0.388809535357882\n",
      "Epoch: 189, step: 0, loss 0.38751803276388513\n",
      "Epoch: 190, step: 0, loss 0.3862324567164781\n",
      "Epoch: 191, step: 0, loss 0.3849527815552608\n",
      "Epoch: 192, step: 0, loss 0.38367898162350517\n",
      "Epoch: 193, step: 0, loss 0.38241103127201254\n",
      "Epoch: 194, step: 0, loss 0.381148904860291\n",
      "Epoch: 195, step: 0, loss 0.37989257675955246\n",
      "Epoch: 196, step: 0, loss 0.3786420213545704\n",
      "Epoch: 197, step: 0, loss 0.37739721304603574\n",
      "Epoch: 198, step: 0, loss 0.37615812625285294\n",
      "Epoch: 199, step: 0, loss 0.37492473541357324\n",
      "Epoch: 200, step: 0, loss 0.3736970149892868\n",
      "Epoch: 201, step: 0, loss 0.3724749394652507\n",
      "Epoch: 202, step: 0, loss 0.371258483352843\n",
      "Epoch: 203, step: 0, loss 0.37004762119147727\n",
      "Epoch: 204, step: 0, loss 0.368842327551038\n",
      "Epoch: 205, step: 0, loss 0.367642577032799\n",
      "Epoch: 206, step: 0, loss 0.36644834427197104\n",
      "Epoch: 207, step: 0, loss 0.3652596039393245\n",
      "Epoch: 208, step: 0, loss 0.364076330742946\n",
      "Epoch: 209, step: 0, loss 0.36289849942976354\n",
      "Epoch: 210, step: 0, loss 0.3617260847873863\n",
      "Epoch: 211, step: 0, loss 0.36055906164579443\n",
      "Epoch: 212, step: 0, loss 0.35939740487883715\n",
      "Epoch: 213, step: 0, loss 0.3582410894059835\n",
      "Epoch: 214, step: 0, loss 0.3570900901933874\n",
      "Epoch: 215, step: 0, loss 0.35594438225602404\n",
      "Epoch: 216, step: 0, loss 0.35480394065879445\n",
      "Epoch: 217, step: 0, loss 0.35366874051787955\n",
      "Epoch: 218, step: 0, loss 0.35253875700225806\n",
      "Epoch: 219, step: 0, loss 0.3514139653351837\n",
      "Epoch: 220, step: 0, loss 0.35029434079538063\n",
      "Epoch: 221, step: 0, loss 0.3491798587183729\n",
      "Epoch: 222, step: 0, loss 0.3480704944976199\n",
      "Epoch: 223, step: 0, loss 0.34696622358633844\n",
      "Epoch: 224, step: 0, loss 0.34586702149768445\n",
      "Epoch: 225, step: 0, loss 0.34477286380679983\n",
      "Epoch: 226, step: 0, loss 0.34368372615160914\n",
      "Epoch: 227, step: 0, loss 0.34259958423404263\n",
      "Epoch: 228, step: 0, loss 0.3415204138210601\n",
      "Epoch: 229, step: 0, loss 0.3404461907455292\n",
      "Epoch: 230, step: 0, loss 0.3393768909079326\n",
      "Epoch: 231, step: 0, loss 0.3383124902763113\n",
      "Epoch: 232, step: 0, loss 0.33725296488816825\n",
      "Epoch: 233, step: 0, loss 0.33619829085084324\n",
      "Epoch: 234, step: 0, loss 0.3351484443427881\n",
      "Epoch: 235, step: 0, loss 0.3341034016145546\n",
      "Epoch: 236, step: 0, loss 0.3330631389888056\n",
      "Epoch: 237, step: 0, loss 0.3320276328622473\n",
      "Epoch: 238, step: 0, loss 0.3309968597061202\n",
      "Epoch: 239, step: 0, loss 0.3299707960664367\n",
      "Epoch: 240, step: 0, loss 0.32894941856546117\n",
      "Epoch: 241, step: 0, loss 0.32793270390191154\n",
      "Epoch: 242, step: 0, loss 0.326920628852239\n",
      "Epoch: 243, step: 0, loss 0.3259131702707473\n",
      "Epoch: 244, step: 0, loss 0.32491030509069485\n",
      "Epoch: 245, step: 0, loss 0.3239120103247105\n",
      "Epoch: 246, step: 0, loss 0.322918263065559\n",
      "Epoch: 247, step: 0, loss 0.32192904048671594\n",
      "Epoch: 248, step: 0, loss 0.3209443198426689\n",
      "Epoch: 249, step: 0, loss 0.3199640784702578\n",
      "Epoch: 250, step: 0, loss 0.31898829378827637\n",
      "Epoch: 251, step: 0, loss 0.318016943298437\n",
      "Epoch: 252, step: 0, loss 0.3170500045860719\n",
      "Epoch: 253, step: 0, loss 0.31608745532024213\n",
      "Epoch: 254, step: 0, loss 0.3151292732545206\n",
      "Epoch: 255, step: 0, loss 0.31417543622711713\n",
      "Epoch: 256, step: 0, loss 0.3132259221615645\n",
      "Epoch: 257, step: 0, loss 0.3122807090670647\n",
      "Epoch: 258, step: 0, loss 0.31133977503866067\n",
      "Epoch: 259, step: 0, loss 0.31040309825815465\n",
      "Epoch: 260, step: 0, loss 0.30947065699377463\n",
      "Epoch: 261, step: 0, loss 0.3085424296010276\n",
      "Epoch: 262, step: 0, loss 0.3076183945227065\n",
      "Epoch: 263, step: 0, loss 0.30669853028947724\n",
      "Epoch: 264, step: 0, loss 0.3057828155197518\n",
      "Epoch: 265, step: 0, loss 0.3048712289207722\n",
      "Epoch: 266, step: 0, loss 0.30396374928753167\n",
      "Epoch: 267, step: 0, loss 0.30306035550450366\n",
      "Epoch: 268, step: 0, loss 0.30216102654487065\n",
      "Epoch: 269, step: 0, loss 0.30126574147093305\n",
      "Epoch: 270, step: 0, loss 0.30037447943465206\n",
      "Epoch: 271, step: 0, loss 0.29948721967734493\n",
      "Epoch: 272, step: 0, loss 0.29860394153027053\n",
      "Epoch: 273, step: 0, loss 0.29772462441466685\n",
      "Epoch: 274, step: 0, loss 0.2968492478414609\n",
      "Epoch: 275, step: 0, loss 0.29597779141226965\n",
      "Epoch: 276, step: 0, loss 0.2951102348188232\n",
      "Epoch: 277, step: 0, loss 0.2942465578430131\n",
      "Epoch: 278, step: 0, loss 0.2933867403577984\n",
      "Epoch: 279, step: 0, loss 0.2925307623259479\n",
      "Epoch: 280, step: 0, loss 0.2916786038016886\n",
      "Epoch: 281, step: 0, loss 0.2908302449293573\n",
      "Epoch: 282, step: 0, loss 0.2899856659441096\n",
      "Epoch: 283, step: 0, loss 0.2891448471723593\n",
      "Epoch: 284, step: 0, loss 0.2883077690307163\n",
      "Epoch: 285, step: 0, loss 0.287474412027003\n",
      "Epoch: 286, step: 0, loss 0.2866447567596894\n",
      "Epoch: 287, step: 0, loss 0.28581878391807825\n",
      "Epoch: 288, step: 0, loss 0.28499647428225594\n",
      "Epoch: 289, step: 0, loss 0.28417780872310106\n",
      "Epoch: 290, step: 0, loss 0.28336276820219086\n",
      "Epoch: 291, step: 0, loss 0.28255133377171654\n",
      "Epoch: 292, step: 0, loss 0.28174348657445747\n",
      "Epoch: 293, step: 0, loss 0.28093920784396914\n",
      "Epoch: 294, step: 0, loss 0.2801384789039003\n",
      "Epoch: 295, step: 0, loss 0.2793412811686421\n",
      "Epoch: 296, step: 0, loss 0.2785475961426681\n",
      "Epoch: 297, step: 0, loss 0.2777574054207414\n",
      "Epoch: 298, step: 0, loss 0.27697069068747543\n",
      "Epoch: 299, step: 0, loss 0.2761874337178989\n",
      "Epoch: 300, step: 0, loss 0.2754076163764497\n",
      "Epoch: 301, step: 0, loss 0.2746312206175535\n",
      "Epoch: 302, step: 0, loss 0.27385822848504404\n",
      "Epoch: 303, step: 0, loss 0.2730886221123728\n",
      "Epoch: 304, step: 0, loss 0.2723223837219122\n",
      "Epoch: 305, step: 0, loss 0.2715594956255813\n",
      "Epoch: 306, step: 0, loss 0.2707999402239487\n",
      "Epoch: 307, step: 0, loss 0.27004370000654154\n",
      "Epoch: 308, step: 0, loss 0.2692907575512819\n",
      "Epoch: 309, step: 0, loss 0.26854109552472244\n",
      "Epoch: 310, step: 0, loss 0.26779469668156475\n",
      "Epoch: 311, step: 0, loss 0.267051543864649\n",
      "Epoch: 312, step: 0, loss 0.2663116200042815\n",
      "Epoch: 313, step: 0, loss 0.2655749081188071\n",
      "Epoch: 314, step: 0, loss 0.26484139131365064\n",
      "Epoch: 315, step: 0, loss 0.2641110527818551\n",
      "Epoch: 316, step: 0, loss 0.2633838758030086\n",
      "Epoch: 317, step: 0, loss 0.26265984374356355\n",
      "Epoch: 318, step: 0, loss 0.2619389400565422\n",
      "Epoch: 319, step: 0, loss 0.2612211482812529\n",
      "Epoch: 320, step: 0, loss 0.2605064520429625\n",
      "Epoch: 321, step: 0, loss 0.2597948350527795\n",
      "Epoch: 322, step: 0, loss 0.25908628110738124\n",
      "Epoch: 323, step: 0, loss 0.258380774088613\n",
      "Epoch: 324, step: 0, loss 0.25767829796337954\n",
      "Epoch: 325, step: 0, loss 0.2569788367834948\n",
      "Epoch: 326, step: 0, loss 0.256282374685039\n",
      "Epoch: 327, step: 0, loss 0.25558889588839745\n",
      "Epoch: 328, step: 0, loss 0.254898384698116\n",
      "Epoch: 329, step: 0, loss 0.25421082550210455\n",
      "Epoch: 330, step: 0, loss 0.25352620277191423\n",
      "Epoch: 331, step: 0, loss 0.2528445010621382\n",
      "Epoch: 332, step: 0, loss 0.252165705010126\n",
      "Epoch: 333, step: 0, loss 0.251489799335905\n",
      "Epoch: 334, step: 0, loss 0.25081676884161747\n",
      "Epoch: 335, step: 0, loss 0.25014659841144654\n",
      "Epoch: 336, step: 0, loss 0.24947927301131728\n",
      "Epoch: 337, step: 0, loss 0.24881477768836\n",
      "Epoch: 338, step: 0, loss 0.24815309757088141\n",
      "Epoch: 339, step: 0, loss 0.24749421786792744\n",
      "Epoch: 340, step: 0, loss 0.24683812386888015\n",
      "Epoch: 341, step: 0, loss 0.24618480094353323\n",
      "Epoch: 342, step: 0, loss 0.2455342345411698\n",
      "Epoch: 343, step: 0, loss 0.24488641019075977\n",
      "Epoch: 344, step: 0, loss 0.2442413135006518\n",
      "Epoch: 345, step: 0, loss 0.243598930157855\n",
      "Epoch: 346, step: 0, loss 0.24295924592797746\n",
      "Epoch: 347, step: 0, loss 0.24232224665500537\n",
      "Epoch: 348, step: 0, loss 0.24168791826073774\n",
      "Epoch: 349, step: 0, loss 0.24105624674460133\n",
      "Epoch: 350, step: 0, loss 0.24042721818335208\n",
      "Epoch: 351, step: 0, loss 0.2398008187307345\n",
      "Epoch: 352, step: 0, loss 0.2391770346170042\n",
      "Epoch: 353, step: 0, loss 0.23855585214885835\n",
      "Epoch: 354, step: 0, loss 0.23793725770890897\n",
      "Epoch: 355, step: 0, loss 0.23732123775528466\n",
      "Epoch: 356, step: 0, loss 0.23670777882176183\n",
      "Epoch: 357, step: 0, loss 0.23609686751671788\n",
      "Epoch: 358, step: 0, loss 0.23548849052349463\n",
      "Epoch: 359, step: 0, loss 0.23488263459945938\n",
      "Epoch: 360, step: 0, loss 0.23427928657620806\n",
      "Epoch: 361, step: 0, loss 0.2336784333589408\n",
      "Epoch: 362, step: 0, loss 0.2330800619259227\n",
      "Epoch: 363, step: 0, loss 0.23248415932872568\n",
      "Epoch: 364, step: 0, loss 0.23189071269137537\n",
      "Epoch: 365, step: 0, loss 0.23129970921019868\n",
      "Epoch: 366, step: 0, loss 0.23071113615340952\n",
      "Epoch: 367, step: 0, loss 0.23012498086109112\n",
      "Epoch: 368, step: 0, loss 0.2295412307444832\n",
      "Epoch: 369, step: 0, loss 0.22895987328557216\n",
      "Epoch: 370, step: 0, loss 0.2283808960371924\n",
      "Epoch: 371, step: 0, loss 0.22780428662235627\n",
      "Epoch: 372, step: 0, loss 0.22723003273402204\n",
      "Epoch: 373, step: 0, loss 0.22665812213473271\n",
      "Epoch: 374, step: 0, loss 0.22608854265609893\n",
      "Epoch: 375, step: 0, loss 0.22552128219889753\n",
      "Epoch: 376, step: 0, loss 0.22495632873240876\n",
      "Epoch: 377, step: 0, loss 0.22439367029406773\n",
      "Epoch: 378, step: 0, loss 0.22383329498903348\n",
      "Epoch: 379, step: 0, loss 0.22327519099040152\n",
      "Epoch: 380, step: 0, loss 0.22271934653809333\n",
      "Epoch: 381, step: 0, loss 0.222165749939162\n",
      "Epoch: 382, step: 0, loss 0.2216143895670551\n",
      "Epoch: 383, step: 0, loss 0.22106525386137146\n",
      "Epoch: 384, step: 0, loss 0.2205183313277312\n",
      "Epoch: 385, step: 0, loss 0.21997361053710546\n",
      "Epoch: 386, step: 0, loss 0.2194310801256047\n",
      "Epoch: 387, step: 0, loss 0.21889072879442767\n",
      "Epoch: 388, step: 0, loss 0.21835254530894133\n",
      "Epoch: 389, step: 0, loss 0.2178165184989909\n",
      "Epoch: 390, step: 0, loss 0.217282637258068\n",
      "Epoch: 391, step: 0, loss 0.2167508905431413\n",
      "Epoch: 392, step: 0, loss 0.21622126737437627\n",
      "Epoch: 393, step: 0, loss 0.21569375683489272\n",
      "Epoch: 394, step: 0, loss 0.21516834807016066\n",
      "Epoch: 395, step: 0, loss 0.21464503028787205\n",
      "Epoch: 396, step: 0, loss 0.21412379275740123\n",
      "Epoch: 397, step: 0, loss 0.2136046248098547\n",
      "Epoch: 398, step: 0, loss 0.21308751583740487\n",
      "Epoch: 399, step: 0, loss 0.21257245529297544\n",
      "Epoch: 400, step: 0, loss 0.212059432690182\n",
      "Epoch: 401, step: 0, loss 0.21154843760269654\n",
      "Epoch: 402, step: 0, loss 0.21103945966398419\n",
      "Epoch: 403, step: 0, loss 0.21053248856711795\n",
      "Epoch: 404, step: 0, loss 0.2100275140645323\n",
      "Epoch: 405, step: 0, loss 0.20952452596733037\n",
      "Epoch: 406, step: 0, loss 0.2090235141452243\n",
      "Epoch: 407, step: 0, loss 0.20852446852616732\n",
      "Epoch: 408, step: 0, loss 0.20802737909597485\n",
      "Epoch: 409, step: 0, loss 0.2075322358982401\n",
      "Epoch: 410, step: 0, loss 0.20703902903358964\n",
      "Epoch: 411, step: 0, loss 0.20654774865975872\n",
      "Epoch: 412, step: 0, loss 0.20605838499118542\n",
      "Epoch: 413, step: 0, loss 0.20557092829820653\n",
      "Epoch: 414, step: 0, loss 0.20508536890782003\n",
      "Epoch: 415, step: 0, loss 0.20460169720214372\n",
      "Epoch: 416, step: 0, loss 0.20411990361890886\n",
      "Epoch: 417, step: 0, loss 0.20363997865096625\n",
      "Epoch: 418, step: 0, loss 0.2031619128458476\n",
      "Epoch: 419, step: 0, loss 0.20268569680556542\n",
      "Epoch: 420, step: 0, loss 0.202211321186103\n",
      "Epoch: 421, step: 0, loss 0.20173877669750373\n",
      "Epoch: 422, step: 0, loss 0.2012680541031191\n",
      "Epoch: 423, step: 0, loss 0.20079914421968906\n",
      "Epoch: 424, step: 0, loss 0.2003320379168104\n",
      "Epoch: 425, step: 0, loss 0.19986672611670653\n",
      "Epoch: 426, step: 0, loss 0.1994031997937467\n",
      "Epoch: 427, step: 0, loss 0.1989414499744946\n",
      "Epoch: 428, step: 0, loss 0.19848146773713837\n",
      "Epoch: 429, step: 0, loss 0.19802324421130782\n",
      "Epoch: 430, step: 0, loss 0.1975667705776952\n",
      "Epoch: 431, step: 0, loss 0.1971120380678066\n",
      "Epoch: 432, step: 0, loss 0.19665903796369608\n",
      "Epoch: 433, step: 0, loss 0.19620776159768005\n",
      "Epoch: 434, step: 0, loss 0.19575820035187994\n",
      "Epoch: 435, step: 0, loss 0.19531034565813893\n",
      "Epoch: 436, step: 0, loss 0.19486418899773703\n",
      "Epoch: 437, step: 0, loss 0.194419721900874\n",
      "Epoch: 438, step: 0, loss 0.19397693594658932\n",
      "Epoch: 439, step: 0, loss 0.19353582276243558\n",
      "Epoch: 440, step: 0, loss 0.19309637402414714\n",
      "Epoch: 441, step: 0, loss 0.19265858145538012\n",
      "Epoch: 442, step: 0, loss 0.19222243682728798\n",
      "Epoch: 443, step: 0, loss 0.19178793195870641\n",
      "Epoch: 444, step: 0, loss 0.19135505871534433\n",
      "Epoch: 445, step: 0, loss 0.19092380900967929\n",
      "Epoch: 446, step: 0, loss 0.19049417480080785\n",
      "Epoch: 447, step: 0, loss 0.19006614809404462\n",
      "Epoch: 448, step: 0, loss 0.18963972094066198\n",
      "Epoch: 449, step: 0, loss 0.1892148854376698\n",
      "Epoch: 450, step: 0, loss 0.1887916337275414\n",
      "Epoch: 451, step: 0, loss 0.1883699579977677\n",
      "Epoch: 452, step: 0, loss 0.18794985048088153\n",
      "Epoch: 453, step: 0, loss 0.1875313034539442\n",
      "Epoch: 454, step: 0, loss 0.18711430923853573\n",
      "Epoch: 455, step: 0, loss 0.18669886020017493\n",
      "Epoch: 456, step: 0, loss 0.18628494874810586\n",
      "Epoch: 457, step: 0, loss 0.18587256733544466\n",
      "Epoch: 458, step: 0, loss 0.18546170845840576\n",
      "Epoch: 459, step: 0, loss 0.18505236465625197\n",
      "Epoch: 460, step: 0, loss 0.18464452851115917\n",
      "Epoch: 461, step: 0, loss 0.18423819264773045\n",
      "Epoch: 462, step: 0, loss 0.18383334973285168\n",
      "Epoch: 463, step: 0, loss 0.18342999247549563\n",
      "Epoch: 464, step: 0, loss 0.18302811362629978\n",
      "Epoch: 465, step: 0, loss 0.18262770597757239\n",
      "Epoch: 466, step: 0, loss 0.1822287623626793\n",
      "Epoch: 467, step: 0, loss 0.18183127565614265\n",
      "Epoch: 468, step: 0, loss 0.18143523877327464\n",
      "Epoch: 469, step: 0, loss 0.18104064466986858\n",
      "Epoch: 470, step: 0, loss 0.18064748634193184\n",
      "Epoch: 471, step: 0, loss 0.18025575682557196\n",
      "Epoch: 472, step: 0, loss 0.1798654491967842\n",
      "Epoch: 473, step: 0, loss 0.17947655657092654\n",
      "Epoch: 474, step: 0, loss 0.17908907210280559\n",
      "Epoch: 475, step: 0, loss 0.17870298898609754\n",
      "Epoch: 476, step: 0, loss 0.1783183004536088\n",
      "Epoch: 477, step: 0, loss 0.1779349997765378\n",
      "Epoch: 478, step: 0, loss 0.17755308026459052\n",
      "Epoch: 479, step: 0, loss 0.17717253526542337\n",
      "Epoch: 480, step: 0, loss 0.176793358164728\n",
      "Epoch: 481, step: 0, loss 0.17641554238586105\n",
      "Epoch: 482, step: 0, loss 0.1760390813894826\n",
      "Epoch: 483, step: 0, loss 0.17566396867356818\n",
      "Epoch: 484, step: 0, loss 0.17529019777316188\n",
      "Epoch: 485, step: 0, loss 0.1749177622599547\n",
      "Epoch: 486, step: 0, loss 0.1745466557421489\n",
      "Epoch: 487, step: 0, loss 0.17417687186424669\n",
      "Epoch: 488, step: 0, loss 0.17380840430691377\n",
      "Epoch: 489, step: 0, loss 0.17344124678670855\n",
      "Epoch: 490, step: 0, loss 0.173075393055705\n",
      "Epoch: 491, step: 0, loss 0.17271083690146616\n",
      "Epoch: 492, step: 0, loss 0.17234757214693058\n",
      "Epoch: 493, step: 0, loss 0.17198559264971325\n",
      "Epoch: 494, step: 0, loss 0.17162489230241088\n",
      "Epoch: 495, step: 0, loss 0.17126546503224876\n",
      "Epoch: 496, step: 0, loss 0.17090730480069594\n",
      "Epoch: 497, step: 0, loss 0.17055040560333914\n",
      "Epoch: 498, step: 0, loss 0.17019476146977916\n",
      "Epoch: 499, step: 0, loss 0.169840366463334\n",
      "Epoch: 500, step: 0, loss 0.16948721468090988\n",
      "Epoch: 501, step: 0, loss 0.16913530025249282\n",
      "Epoch: 502, step: 0, loss 0.16878461734146719\n",
      "Epoch: 503, step: 0, loss 0.1684351601439595\n",
      "Epoch: 504, step: 0, loss 0.16808692288888136\n",
      "Epoch: 505, step: 0, loss 0.16773989983755672\n",
      "Epoch: 506, step: 0, loss 0.16739408528365346\n",
      "Epoch: 507, step: 0, loss 0.16704947355300853\n",
      "Epoch: 508, step: 0, loss 0.1667060590033385\n",
      "Epoch: 509, step: 0, loss 0.16636383602397692\n",
      "Epoch: 510, step: 0, loss 0.1660227990359219\n",
      "Epoch: 511, step: 0, loss 0.1656829424914874\n",
      "Epoch: 512, step: 0, loss 0.16534426087394882\n",
      "Epoch: 513, step: 0, loss 0.16500674869765314\n",
      "Epoch: 514, step: 0, loss 0.16467040050773693\n",
      "Epoch: 515, step: 0, loss 0.16433521087980751\n",
      "Epoch: 516, step: 0, loss 0.16400117441983383\n",
      "Epoch: 517, step: 0, loss 0.16366828576409811\n",
      "Epoch: 518, step: 0, loss 0.16333653957869848\n",
      "Epoch: 519, step: 0, loss 0.1630059305597224\n",
      "Epoch: 520, step: 0, loss 0.16267645343270093\n",
      "Epoch: 521, step: 0, loss 0.16234810295286214\n",
      "Epoch: 522, step: 0, loss 0.16202087390450953\n",
      "Epoch: 523, step: 0, loss 0.16169476110103764\n",
      "Epoch: 524, step: 0, loss 0.16136975938489978\n",
      "Epoch: 525, step: 0, loss 0.1610458636270627\n",
      "Epoch: 526, step: 0, loss 0.16072306872727174\n",
      "Epoch: 527, step: 0, loss 0.16040136961346468\n",
      "Epoch: 528, step: 0, loss 0.16008076124191492\n",
      "Epoch: 529, step: 0, loss 0.15976123859683863\n",
      "Epoch: 530, step: 0, loss 0.15944279669030628\n",
      "Epoch: 531, step: 0, loss 0.15912543056211023\n",
      "Epoch: 532, step: 0, loss 0.15880913527957713\n",
      "Epoch: 533, step: 0, loss 0.1584939059373168\n",
      "Epoch: 534, step: 0, loss 0.15817973765723883\n",
      "Epoch: 535, step: 0, loss 0.15786662558811942\n",
      "Epoch: 536, step: 0, loss 0.15755456490561234\n",
      "Epoch: 537, step: 0, loss 0.1572435508120435\n",
      "Epoch: 538, step: 0, loss 0.15693357853632697\n",
      "Epoch: 539, step: 0, loss 0.15662464333370266\n",
      "Epoch: 540, step: 0, loss 0.15631674048549019\n",
      "Epoch: 541, step: 0, loss 0.15600986529922312\n",
      "Epoch: 542, step: 0, loss 0.15570401310808551\n",
      "Epoch: 543, step: 0, loss 0.15539917927110472\n",
      "Epoch: 544, step: 0, loss 0.1550953591728651\n",
      "Epoch: 545, step: 0, loss 0.15479254822329286\n",
      "Epoch: 546, step: 0, loss 0.15449074185760298\n",
      "Epoch: 547, step: 0, loss 0.1541899355360451\n",
      "Epoch: 548, step: 0, loss 0.15389012474374472\n",
      "Epoch: 549, step: 0, loss 0.1535913049907352\n",
      "Epoch: 550, step: 0, loss 0.15329347181156955\n",
      "Epoch: 551, step: 0, loss 0.15299662076533097\n",
      "Epoch: 552, step: 0, loss 0.1527007474352654\n",
      "Epoch: 553, step: 0, loss 0.15240584742904856\n",
      "Epoch: 554, step: 0, loss 0.15211191637823218\n",
      "Epoch: 555, step: 0, loss 0.15181894993818673\n",
      "Epoch: 556, step: 0, loss 0.15152694378810605\n",
      "Epoch: 557, step: 0, loss 0.15123589363078896\n",
      "Epoch: 558, step: 0, loss 0.15094579519236215\n",
      "Epoch: 559, step: 0, loss 0.15065664422226444\n",
      "Epoch: 560, step: 0, loss 0.1503684364931818\n",
      "Epoch: 561, step: 0, loss 0.15008116780078742\n",
      "Epoch: 562, step: 0, loss 0.14979483396353516\n",
      "Epoch: 563, step: 0, loss 0.14950943082265228\n",
      "Epoch: 564, step: 0, loss 0.14922495424196594\n",
      "Epoch: 565, step: 0, loss 0.14894140010769957\n",
      "Epoch: 566, step: 0, loss 0.14865876432854588\n",
      "Epoch: 567, step: 0, loss 0.14837704283514488\n",
      "Epoch: 568, step: 0, loss 0.14809623158035043\n",
      "Epoch: 569, step: 0, loss 0.14781632653883536\n",
      "Epoch: 570, step: 0, loss 0.14753732370709538\n",
      "Epoch: 571, step: 0, loss 0.1472592191032294\n",
      "Epoch: 572, step: 0, loss 0.14698200876687362\n",
      "Epoch: 573, step: 0, loss 0.14670568875900303\n",
      "Epoch: 574, step: 0, loss 0.14643025516183514\n",
      "Epoch: 575, step: 0, loss 0.14615570407877912\n",
      "Epoch: 576, step: 0, loss 0.1458820316339981\n",
      "Epoch: 577, step: 0, loss 0.14560923397284906\n",
      "Epoch: 578, step: 0, loss 0.1453373072611578\n",
      "Epoch: 579, step: 0, loss 0.14506624768553514\n",
      "Epoch: 580, step: 0, loss 0.14479605145289653\n",
      "Epoch: 581, step: 0, loss 0.1445267147906098\n",
      "Epoch: 582, step: 0, loss 0.14425823394624993\n",
      "Epoch: 583, step: 0, loss 0.14399060518752266\n",
      "Epoch: 584, step: 0, loss 0.14372382480203846\n",
      "Epoch: 585, step: 0, loss 0.14345788909738158\n",
      "Epoch: 586, step: 0, loss 0.1431927944008065\n",
      "Epoch: 587, step: 0, loss 0.14292853705913544\n",
      "Epoch: 588, step: 0, loss 0.14266511343882307\n",
      "Epoch: 589, step: 0, loss 0.142402519925601\n",
      "Epoch: 590, step: 0, loss 0.1421407529245331\n",
      "Epoch: 591, step: 0, loss 0.1418798088597444\n",
      "Epoch: 592, step: 0, loss 0.14161968417447762\n",
      "Epoch: 593, step: 0, loss 0.1413603753309032\n",
      "Epoch: 594, step: 0, loss 0.14110187880991415\n",
      "Epoch: 595, step: 0, loss 0.1408441911110698\n",
      "Epoch: 596, step: 0, loss 0.14058730875260436\n",
      "Epoch: 597, step: 0, loss 0.1403312282711824\n",
      "Epoch: 598, step: 0, loss 0.14007594622170694\n",
      "Epoch: 599, step: 0, loss 0.13982145917751115\n",
      "Epoch: 600, step: 0, loss 0.1395677637299083\n",
      "Epoch: 601, step: 0, loss 0.1393148564883084\n",
      "Epoch: 602, step: 0, loss 0.13906273407997402\n",
      "Epoch: 603, step: 0, loss 0.13881139314998428\n",
      "Epoch: 604, step: 0, loss 0.13856083036110825\n",
      "Epoch: 605, step: 0, loss 0.13831104239375078\n",
      "Epoch: 606, step: 0, loss 0.13806202594580394\n",
      "Epoch: 607, step: 0, loss 0.13781377773239656\n",
      "Epoch: 608, step: 0, loss 0.1375662944859942\n",
      "Epoch: 609, step: 0, loss 0.13731957295629924\n",
      "Epoch: 610, step: 0, loss 0.1370736099101325\n",
      "Epoch: 611, step: 0, loss 0.13682840213100517\n",
      "Epoch: 612, step: 0, loss 0.13658394641952945\n",
      "Epoch: 613, step: 0, loss 0.13634023959299407\n",
      "Epoch: 614, step: 0, loss 0.1360972784853445\n",
      "Epoch: 615, step: 0, loss 0.1358550599470962\n",
      "Epoch: 616, step: 0, loss 0.1356135808451693\n",
      "Epoch: 617, step: 0, loss 0.13537283806285613\n",
      "Epoch: 618, step: 0, loss 0.1351328284997566\n",
      "Epoch: 619, step: 0, loss 0.1348935490716456\n",
      "Epoch: 620, step: 0, loss 0.13465499671023018\n",
      "Epoch: 621, step: 0, loss 0.13441716836335427\n",
      "Epoch: 622, step: 0, loss 0.13418006099458207\n",
      "Epoch: 623, step: 0, loss 0.13394367158337014\n",
      "Epoch: 624, step: 0, loss 0.1337079971247543\n",
      "Epoch: 625, step: 0, loss 0.13347303462949733\n",
      "Epoch: 626, step: 0, loss 0.13323878112370255\n",
      "Epoch: 627, step: 0, loss 0.13300523364900604\n",
      "Epoch: 628, step: 0, loss 0.13277238926225013\n",
      "Epoch: 629, step: 0, loss 0.13254024503558307\n",
      "Epoch: 630, step: 0, loss 0.1323087980562111\n",
      "Epoch: 631, step: 0, loss 0.1320780454264469\n",
      "Epoch: 632, step: 0, loss 0.13184798426348485\n",
      "Epoch: 633, step: 0, loss 0.1316186116994296\n",
      "Epoch: 634, step: 0, loss 0.13138992488113205\n",
      "Epoch: 635, step: 0, loss 0.13116192097005655\n",
      "Epoch: 636, step: 0, loss 0.13093459714245323\n",
      "Epoch: 637, step: 0, loss 0.13070795058888554\n",
      "Epoch: 638, step: 0, loss 0.13048197851444945\n",
      "Epoch: 639, step: 0, loss 0.1302566781385311\n",
      "Epoch: 640, step: 0, loss 0.13003204669480817\n",
      "Epoch: 641, step: 0, loss 0.12980808143106345\n",
      "Epoch: 642, step: 0, loss 0.12958477960925527\n",
      "Epoch: 643, step: 0, loss 0.1293621385052021\n",
      "Epoch: 644, step: 0, loss 0.1291401554088639\n",
      "Epoch: 645, step: 0, loss 0.12891882762374668\n",
      "Epoch: 646, step: 0, loss 0.12869815246735977\n",
      "Epoch: 647, step: 0, loss 0.12847812727072058\n",
      "Epoch: 648, step: 0, loss 0.1282587493785656\n",
      "Epoch: 649, step: 0, loss 0.1280400161490208\n",
      "Epoch: 650, step: 0, loss 0.12782192495369382\n",
      "Epoch: 651, step: 0, loss 0.127604473177589\n",
      "Epoch: 652, step: 0, loss 0.12738765821890954\n",
      "Epoch: 653, step: 0, loss 0.12717147748915464\n",
      "Epoch: 654, step: 0, loss 0.12695592841275935\n",
      "Epoch: 655, step: 0, loss 0.12674100842741032\n",
      "Epoch: 656, step: 0, loss 0.12652671498359622\n",
      "Epoch: 657, step: 0, loss 0.12631304554478975\n",
      "Epoch: 658, step: 0, loss 0.12609999758722729\n",
      "Epoch: 659, step: 0, loss 0.12588756859989378\n",
      "Epoch: 660, step: 0, loss 0.12567575608442874\n",
      "Epoch: 661, step: 0, loss 0.125464557555023\n",
      "Epoch: 662, step: 0, loss 0.12525397053850182\n",
      "Epoch: 663, step: 0, loss 0.1250439925740061\n",
      "Epoch: 664, step: 0, loss 0.12483462121305522\n",
      "Epoch: 665, step: 0, loss 0.1246258540195406\n",
      "Epoch: 666, step: 0, loss 0.12441768856948021\n",
      "Epoch: 667, step: 0, loss 0.12421012245115123\n",
      "Epoch: 668, step: 0, loss 0.12400315326478625\n",
      "Epoch: 669, step: 0, loss 0.12379677862274163\n",
      "Epoch: 670, step: 0, loss 0.1235909961492809\n",
      "Epoch: 671, step: 0, loss 0.12338580348047791\n",
      "Epoch: 672, step: 0, loss 0.12318119826425256\n",
      "Epoch: 673, step: 0, loss 0.12297717816025854\n",
      "Epoch: 674, step: 0, loss 0.12277374083984631\n",
      "Epoch: 675, step: 0, loss 0.12257088398584891\n",
      "Epoch: 676, step: 0, loss 0.12236860529279037\n",
      "Epoch: 677, step: 0, loss 0.12216690246652871\n",
      "Epoch: 678, step: 0, loss 0.1219657732243786\n",
      "Epoch: 679, step: 0, loss 0.12176521529493152\n",
      "Epoch: 680, step: 0, loss 0.12156522641805523\n",
      "Epoch: 681, step: 0, loss 0.12136580434493058\n",
      "Epoch: 682, step: 0, loss 0.12116694683763857\n",
      "Epoch: 683, step: 0, loss 0.12096865166960655\n",
      "Epoch: 684, step: 0, loss 0.12077091662503087\n",
      "Epoch: 685, step: 0, loss 0.12057373949921354\n",
      "Epoch: 686, step: 0, loss 0.1203771180982188\n",
      "Epoch: 687, step: 0, loss 0.12018105023897953\n",
      "Epoch: 688, step: 0, loss 0.1199855337491467\n",
      "Epoch: 689, step: 0, loss 0.11979056646711868\n",
      "Epoch: 690, step: 0, loss 0.11959614624193488\n",
      "Epoch: 691, step: 0, loss 0.11940227093306549\n",
      "Epoch: 692, step: 0, loss 0.1192089384106609\n",
      "Epoch: 693, step: 0, loss 0.11901614655525843\n",
      "Epoch: 694, step: 0, loss 0.1188238932576612\n",
      "Epoch: 695, step: 0, loss 0.1186321764192291\n",
      "Epoch: 696, step: 0, loss 0.11844099395129863\n",
      "Epoch: 697, step: 0, loss 0.11825034377575816\n",
      "Epoch: 698, step: 0, loss 0.11806022382439009\n",
      "Epoch: 699, step: 0, loss 0.1178706320391387\n",
      "Epoch: 700, step: 0, loss 0.11768156637202139\n",
      "Epoch: 701, step: 0, loss 0.11749302478497747\n",
      "Epoch: 702, step: 0, loss 0.11730500524993254\n",
      "Epoch: 703, step: 0, loss 0.11711750574862424\n",
      "Epoch: 704, step: 0, loss 0.11693052427255846\n",
      "Epoch: 705, step: 0, loss 0.116744058823086\n",
      "Epoch: 706, step: 0, loss 0.11655810741121335\n",
      "Epoch: 707, step: 0, loss 0.11637266805759387\n",
      "Epoch: 708, step: 0, loss 0.116187738792411\n",
      "Epoch: 709, step: 0, loss 0.11600331765544372\n",
      "Epoch: 710, step: 0, loss 0.11581940269595961\n",
      "Epoch: 711, step: 0, loss 0.11563599197257635\n",
      "Epoch: 712, step: 0, loss 0.11545308355336349\n",
      "Epoch: 713, step: 0, loss 0.11527067551557402\n",
      "Epoch: 714, step: 0, loss 0.11508876594597739\n",
      "Epoch: 715, step: 0, loss 0.11490735294029751\n",
      "Epoch: 716, step: 0, loss 0.11472643460344407\n",
      "Epoch: 717, step: 0, loss 0.11454600904962962\n",
      "Epoch: 718, step: 0, loss 0.11436607440193061\n",
      "Epoch: 719, step: 0, loss 0.11418662879252475\n",
      "Epoch: 720, step: 0, loss 0.1140076703624625\n",
      "Epoch: 721, step: 0, loss 0.11382919726180422\n",
      "Epoch: 722, step: 0, loss 0.11365120764940276\n",
      "Epoch: 723, step: 0, loss 0.11347369969283512\n",
      "Epoch: 724, step: 0, loss 0.11329667156858755\n",
      "Epoch: 725, step: 0, loss 0.11312012146172466\n",
      "Epoch: 726, step: 0, loss 0.11294404756606873\n",
      "Epoch: 727, step: 0, loss 0.1127684480839598\n",
      "Epoch: 728, step: 0, loss 0.11259332122639477\n",
      "Epoch: 729, step: 0, loss 0.11241866521272127\n",
      "Epoch: 730, step: 0, loss 0.11224447827087818\n",
      "Epoch: 731, step: 0, loss 0.11207075863719464\n",
      "Epoch: 732, step: 0, loss 0.11189750455639143\n",
      "Epoch: 733, step: 0, loss 0.1117247142814354\n",
      "Epoch: 734, step: 0, loss 0.11155238607365048\n",
      "Epoch: 735, step: 0, loss 0.11138051820250142\n",
      "Epoch: 736, step: 0, loss 0.1112091089457145\n",
      "Epoch: 737, step: 0, loss 0.11103815658910475\n",
      "Epoch: 738, step: 0, loss 0.11086765942660412\n",
      "Epoch: 739, step: 0, loss 0.11069761576018389\n",
      "Epoch: 740, step: 0, loss 0.11052802389979645\n",
      "Epoch: 741, step: 0, loss 0.1103588821633761\n",
      "Epoch: 742, step: 0, loss 0.11019018887676092\n",
      "Epoch: 743, step: 0, loss 0.11002194237365098\n",
      "Epoch: 744, step: 0, loss 0.10985414099558245\n",
      "Epoch: 745, step: 0, loss 0.10968678309191704\n",
      "Epoch: 746, step: 0, loss 0.10951986701960086\n",
      "Epoch: 747, step: 0, loss 0.10935339114351388\n",
      "Epoch: 748, step: 0, loss 0.10918735383597415\n",
      "Epoch: 749, step: 0, loss 0.109021753476934\n",
      "Epoch: 750, step: 0, loss 0.10885658845406973\n",
      "Epoch: 751, step: 0, loss 0.10869185716248285\n",
      "Epoch: 752, step: 0, loss 0.10852755800472431\n",
      "Epoch: 753, step: 0, loss 0.10836368939078478\n",
      "Epoch: 754, step: 0, loss 0.1082002497381559\n",
      "Epoch: 755, step: 0, loss 0.10803723747163416\n",
      "Epoch: 756, step: 0, loss 0.10787465102337337\n",
      "Epoch: 757, step: 0, loss 0.10771248883268175\n",
      "Epoch: 758, step: 0, loss 0.10755074934622007\n",
      "Epoch: 759, step: 0, loss 0.10738943101787804\n",
      "Epoch: 760, step: 0, loss 0.1072285323085992\n",
      "Epoch: 761, step: 0, loss 0.1070680516865968\n",
      "Epoch: 762, step: 0, loss 0.10690798762698146\n",
      "Epoch: 763, step: 0, loss 0.1067483386120508\n",
      "Epoch: 764, step: 0, loss 0.1065891031310536\n",
      "Epoch: 765, step: 0, loss 0.10643027968019182\n",
      "Epoch: 766, step: 0, loss 0.10627186676264481\n",
      "Epoch: 767, step: 0, loss 0.106113862888452\n",
      "Epoch: 768, step: 0, loss 0.10595626657451147\n",
      "Epoch: 769, step: 0, loss 0.10579907634452247\n",
      "Epoch: 770, step: 0, loss 0.10564229072903637\n",
      "Epoch: 771, step: 0, loss 0.10548590826519665\n",
      "Epoch: 772, step: 0, loss 0.10532992749707641\n",
      "Epoch: 773, step: 0, loss 0.10517434697511972\n",
      "Epoch: 774, step: 0, loss 0.10501916525668534\n",
      "Epoch: 775, step: 0, loss 0.10486438090558936\n",
      "Epoch: 776, step: 0, loss 0.10470999249224033\n",
      "Epoch: 777, step: 0, loss 0.10455599859352487\n",
      "Epoch: 778, step: 0, loss 0.10440239779294533\n",
      "Epoch: 779, step: 0, loss 0.10424918868030283\n",
      "Epoch: 780, step: 0, loss 0.1040963698519451\n",
      "Epoch: 781, step: 0, loss 0.10394393991046467\n",
      "Epoch: 782, step: 0, loss 0.103791897464962\n",
      "Epoch: 783, step: 0, loss 0.1036402411307593\n",
      "Epoch: 784, step: 0, loss 0.10348896952950148\n",
      "Epoch: 785, step: 0, loss 0.1033380812890777\n",
      "Epoch: 786, step: 0, loss 0.10318757504348182\n",
      "Epoch: 787, step: 0, loss 0.10303744943310315\n",
      "Epoch: 788, step: 0, loss 0.10288770310432876\n",
      "Epoch: 789, step: 0, loss 0.10273833470965726\n",
      "Epoch: 790, step: 0, loss 0.10258934290772909\n",
      "Epoch: 791, step: 0, loss 0.1024407263632622\n",
      "Epoch: 792, step: 0, loss 0.1022924837468741\n",
      "Epoch: 793, step: 0, loss 0.10214461373525985\n",
      "Epoch: 794, step: 0, loss 0.10199711501101631\n",
      "Epoch: 795, step: 0, loss 0.10184998626271687\n",
      "Epoch: 796, step: 0, loss 0.10170322618478722\n",
      "Epoch: 797, step: 0, loss 0.10155683347747489\n",
      "Epoch: 798, step: 0, loss 0.1014108068469918\n",
      "Epoch: 799, step: 0, loss 0.10126514500517406\n",
      "Epoch: 800, step: 0, loss 0.10111984666976026\n",
      "Epoch: 801, step: 0, loss 0.10097491056405064\n",
      "Epoch: 802, step: 0, loss 0.10083033541726974\n",
      "Epoch: 803, step: 0, loss 0.10068611996416338\n",
      "Epoch: 804, step: 0, loss 0.10054226294516178\n",
      "Epoch: 805, step: 0, loss 0.10039876310631284\n",
      "Epoch: 806, step: 0, loss 0.10025561919920395\n",
      "Epoch: 807, step: 0, loss 0.10011282998105664\n",
      "Epoch: 808, step: 0, loss 0.09997039421455735\n",
      "Epoch: 809, step: 0, loss 0.09982831066790801\n",
      "Epoch: 810, step: 0, loss 0.09968657811477034\n",
      "Epoch: 811, step: 0, loss 0.09954519533423405\n",
      "Epoch: 812, step: 0, loss 0.09940416111080047\n",
      "Epoch: 813, step: 0, loss 0.09926347423439298\n",
      "Epoch: 814, step: 0, loss 0.0991231335002095\n",
      "Epoch: 815, step: 0, loss 0.0989831377088235\n",
      "Epoch: 816, step: 0, loss 0.09884348566610615\n",
      "Epoch: 817, step: 0, loss 0.09870417618310733\n",
      "Epoch: 818, step: 0, loss 0.09856520807628302\n",
      "Epoch: 819, step: 0, loss 0.09842658016715143\n",
      "Epoch: 820, step: 0, loss 0.09828829128249972\n",
      "Epoch: 821, step: 0, loss 0.09815034025418715\n",
      "Epoch: 822, step: 0, loss 0.09801272591927261\n",
      "Epoch: 823, step: 0, loss 0.09787544711989855\n",
      "Epoch: 824, step: 0, loss 0.09773850270332657\n",
      "Epoch: 825, step: 0, loss 0.0976018915217957\n",
      "Epoch: 826, step: 0, loss 0.09746561243261886\n",
      "Epoch: 827, step: 0, loss 0.09732966429806399\n",
      "Epoch: 828, step: 0, loss 0.09719404598535857\n",
      "Epoch: 829, step: 0, loss 0.09705875636678701\n",
      "Epoch: 830, step: 0, loss 0.09692379431942978\n",
      "Epoch: 831, step: 0, loss 0.0967891587252861\n",
      "Epoch: 832, step: 0, loss 0.0966548484712627\n",
      "Epoch: 833, step: 0, loss 0.0965208624491238\n",
      "Epoch: 834, step: 0, loss 0.09638719955536716\n",
      "Epoch: 835, step: 0, loss 0.09625385869135268\n",
      "Epoch: 836, step: 0, loss 0.09612083876317269\n",
      "Epoch: 837, step: 0, loss 0.09598813868171545\n",
      "Epoch: 838, step: 0, loss 0.09585575736250443\n",
      "Epoch: 839, step: 0, loss 0.09572369372579563\n",
      "Epoch: 840, step: 0, loss 0.09559194669655281\n",
      "Epoch: 841, step: 0, loss 0.09546051520431585\n",
      "Epoch: 842, step: 0, loss 0.09532939818334436\n",
      "Epoch: 843, step: 0, loss 0.09519859457236492\n",
      "Epoch: 844, step: 0, loss 0.09506810331477533\n",
      "Epoch: 845, step: 0, loss 0.09493792335852216\n",
      "Epoch: 846, step: 0, loss 0.0948080536560088\n",
      "Epoch: 847, step: 0, loss 0.0946784931641748\n",
      "Epoch: 848, step: 0, loss 0.09454924084448069\n",
      "Epoch: 849, step: 0, loss 0.0944202956627731\n",
      "Epoch: 850, step: 0, loss 0.09429165658940787\n",
      "Epoch: 851, step: 0, loss 0.09416332259911608\n",
      "Epoch: 852, step: 0, loss 0.09403529267099404\n",
      "Epoch: 853, step: 0, loss 0.09390756578851255\n",
      "Epoch: 854, step: 0, loss 0.0937801409395505\n",
      "Epoch: 855, step: 0, loss 0.09365301711622916\n",
      "Epoch: 856, step: 0, loss 0.09352619331501778\n",
      "Epoch: 857, step: 0, loss 0.0933996685366465\n",
      "Epoch: 858, step: 0, loss 0.09327344178607147\n",
      "Epoch: 859, step: 0, loss 0.09314751207253551\n",
      "Epoch: 860, step: 0, loss 0.09302187840945643\n",
      "Epoch: 861, step: 0, loss 0.0928965398144482\n",
      "Epoch: 862, step: 0, loss 0.09277149530933727\n",
      "Epoch: 863, step: 0, loss 0.09264674392003647\n",
      "Epoch: 864, step: 0, loss 0.09252228467660101\n",
      "Epoch: 865, step: 0, loss 0.09239811661323524\n",
      "Epoch: 866, step: 0, loss 0.09227423876819153\n",
      "Epoch: 867, step: 0, loss 0.0921506501837741\n",
      "Epoch: 868, step: 0, loss 0.09202734990635437\n",
      "Epoch: 869, step: 0, loss 0.09190433698633056\n",
      "Epoch: 870, step: 0, loss 0.09178161047811578\n",
      "Epoch: 871, step: 0, loss 0.09165916944009407\n",
      "Epoch: 872, step: 0, loss 0.09153701293458354\n",
      "Epoch: 873, step: 0, loss 0.09141514002784627\n",
      "Epoch: 874, step: 0, loss 0.09129354979011872\n",
      "Epoch: 875, step: 0, loss 0.09117224129552835\n",
      "Epoch: 876, step: 0, loss 0.09105121362203852\n",
      "Epoch: 877, step: 0, loss 0.09093046585149327\n",
      "Epoch: 878, step: 0, loss 0.09080999706963089\n",
      "Epoch: 879, step: 0, loss 0.09068980636599391\n",
      "Epoch: 880, step: 0, loss 0.09056989283389594\n",
      "Epoch: 881, step: 0, loss 0.09045025557043423\n",
      "Epoch: 882, step: 0, loss 0.09033089367651517\n",
      "Epoch: 883, step: 0, loss 0.09021180625678579\n",
      "Epoch: 884, step: 0, loss 0.09009299241961431\n",
      "Epoch: 885, step: 0, loss 0.08997445127705012\n",
      "Epoch: 886, step: 0, loss 0.08985618194490157\n",
      "Epoch: 887, step: 0, loss 0.08973818354260837\n",
      "Epoch: 888, step: 0, loss 0.0896204551932646\n",
      "Epoch: 889, step: 0, loss 0.08950299602364155\n",
      "Epoch: 890, step: 0, loss 0.0893858051640579\n",
      "Epoch: 891, step: 0, loss 0.0892688817484921\n",
      "Epoch: 892, step: 0, loss 0.0891522249145215\n",
      "Epoch: 893, step: 0, loss 0.08903583380320185\n",
      "Epoch: 894, step: 0, loss 0.08891970755927475\n",
      "Epoch: 895, step: 0, loss 0.08880384533089479\n",
      "Epoch: 896, step: 0, loss 0.08868824626972752\n",
      "Epoch: 897, step: 0, loss 0.08857290953101699\n",
      "Epoch: 898, step: 0, loss 0.08845783427342734\n",
      "Epoch: 899, step: 0, loss 0.08834301965913834\n",
      "Epoch: 900, step: 0, loss 0.0882284648536802\n",
      "Epoch: 901, step: 0, loss 0.08811416902612793\n",
      "Epoch: 902, step: 0, loss 0.08800013134883602\n",
      "Epoch: 903, step: 0, loss 0.08788635099767642\n",
      "Epoch: 904, step: 0, loss 0.08777282715182474\n",
      "Epoch: 905, step: 0, loss 0.08765955899378608\n",
      "Epoch: 906, step: 0, loss 0.08754654570956372\n",
      "Epoch: 907, step: 0, loss 0.08743378648828423\n",
      "Epoch: 908, step: 0, loss 0.0873212805225192\n",
      "Epoch: 909, step: 0, loss 0.08720902700808886\n",
      "Epoch: 910, step: 0, loss 0.0870970251440931\n",
      "Epoch: 911, step: 0, loss 0.08698527413291952\n",
      "Epoch: 912, step: 0, loss 0.0868737731801673\n",
      "Epoch: 913, step: 0, loss 0.08676252149470727\n",
      "Epoch: 914, step: 0, loss 0.08665151828855192\n",
      "Epoch: 915, step: 0, loss 0.08654076277700791\n",
      "Epoch: 916, step: 0, loss 0.08643025417848496\n",
      "Epoch: 917, step: 0, loss 0.08631999171457942\n",
      "Epoch: 918, step: 0, loss 0.08620997461008637\n",
      "Epoch: 919, step: 0, loss 0.08610020209291296\n",
      "Epoch: 920, step: 0, loss 0.08599067339401291\n",
      "Epoch: 921, step: 0, loss 0.0858813877475653\n",
      "Epoch: 922, step: 0, loss 0.08577234439075755\n",
      "Epoch: 923, step: 0, loss 0.08566354256388181\n",
      "Epoch: 924, step: 0, loss 0.08555498151031223\n",
      "Epoch: 925, step: 0, loss 0.08544666047641113\n",
      "Epoch: 926, step: 0, loss 0.08533857871163048\n",
      "Epoch: 927, step: 0, loss 0.08523073546843014\n",
      "Epoch: 928, step: 0, loss 0.08512313000221447\n",
      "Epoch: 929, step: 0, loss 0.08501576157145062\n",
      "Epoch: 930, step: 0, loss 0.08490862943758218\n",
      "Epoch: 931, step: 0, loss 0.0848017328649026\n",
      "Epoch: 932, step: 0, loss 0.08469507112070472\n",
      "Epoch: 933, step: 0, loss 0.0845886434753222\n",
      "Epoch: 934, step: 0, loss 0.08448244920188669\n",
      "Epoch: 935, step: 0, loss 0.08437648757642066\n",
      "Epoch: 936, step: 0, loss 0.08427075787787776\n",
      "Epoch: 937, step: 0, loss 0.0841652593881138\n",
      "Epoch: 938, step: 0, loss 0.08405999139180781\n",
      "Epoch: 939, step: 0, loss 0.08395495317645592\n",
      "Epoch: 940, step: 0, loss 0.08385014403241957\n",
      "Epoch: 941, step: 0, loss 0.08374556325288834\n",
      "Epoch: 942, step: 0, loss 0.08364121013387665\n",
      "Epoch: 943, step: 0, loss 0.08353708397411964\n",
      "Epoch: 944, step: 0, loss 0.0834331840751907\n",
      "Epoch: 945, step: 0, loss 0.08332950974144018\n",
      "Epoch: 946, step: 0, loss 0.08322606027991976\n",
      "Epoch: 947, step: 0, loss 0.08312283500046007\n",
      "Epoch: 948, step: 0, loss 0.08301983321554247\n",
      "Epoch: 949, step: 0, loss 0.08291705424049949\n",
      "Epoch: 950, step: 0, loss 0.08281449739322766\n",
      "Epoch: 951, step: 0, loss 0.08271216199442785\n",
      "Epoch: 952, step: 0, loss 0.08261004736737189\n",
      "Epoch: 953, step: 0, loss 0.08250815283805797\n",
      "Epoch: 954, step: 0, loss 0.08240647773511886\n",
      "Epoch: 955, step: 0, loss 0.08230502138979243\n",
      "Epoch: 956, step: 0, loss 0.08220378313596635\n",
      "Epoch: 957, step: 0, loss 0.08210276231015669\n",
      "Epoch: 958, step: 0, loss 0.08200195825148114\n",
      "Epoch: 959, step: 0, loss 0.08190137030163581\n",
      "Epoch: 960, step: 0, loss 0.08180099780481624\n",
      "Epoch: 961, step: 0, loss 0.08170084010793449\n",
      "Epoch: 962, step: 0, loss 0.0816008965602962\n",
      "Epoch: 963, step: 0, loss 0.08150116651383386\n",
      "Epoch: 964, step: 0, loss 0.08140164932299766\n",
      "Epoch: 965, step: 0, loss 0.08130234434475558\n",
      "Epoch: 966, step: 0, loss 0.08120325093852368\n",
      "Epoch: 967, step: 0, loss 0.08110436846633563\n",
      "Epoch: 968, step: 0, loss 0.08100569629252773\n",
      "Epoch: 969, step: 0, loss 0.08090723378400168\n",
      "Epoch: 970, step: 0, loss 0.08080898031018327\n",
      "Epoch: 971, step: 0, loss 0.08071093524281044\n",
      "Epoch: 972, step: 0, loss 0.08061309795613607\n",
      "Epoch: 973, step: 0, loss 0.08051546782679504\n",
      "Epoch: 974, step: 0, loss 0.0804180442338716\n",
      "Epoch: 975, step: 0, loss 0.08032082655880872\n",
      "Epoch: 976, step: 0, loss 0.08022381418544187\n",
      "Epoch: 977, step: 0, loss 0.08012700650002468\n",
      "Epoch: 978, step: 0, loss 0.08003040289112484\n",
      "Epoch: 979, step: 0, loss 0.07993400274973281\n",
      "Epoch: 980, step: 0, loss 0.07983780546911551\n",
      "Epoch: 981, step: 0, loss 0.07974181044489172\n",
      "Epoch: 982, step: 0, loss 0.07964601707499479\n",
      "Epoch: 983, step: 0, loss 0.07955042475966484\n",
      "Epoch: 984, step: 0, loss 0.07945503290151573\n",
      "Epoch: 985, step: 0, loss 0.07935984090534098\n",
      "Epoch: 986, step: 0, loss 0.07926484817830588\n",
      "Epoch: 987, step: 0, loss 0.07917005412979118\n",
      "Epoch: 988, step: 0, loss 0.0790754581714247\n",
      "Epoch: 989, step: 0, loss 0.07898105971712345\n",
      "Epoch: 990, step: 0, loss 0.07888685818306912\n",
      "Epoch: 991, step: 0, loss 0.07879285298752123\n",
      "Epoch: 992, step: 0, loss 0.07869904355112355\n",
      "Epoch: 993, step: 0, loss 0.07860542929671115\n",
      "Epoch: 994, step: 0, loss 0.07851200964916608\n",
      "Epoch: 995, step: 0, loss 0.07841878403570224\n",
      "Epoch: 996, step: 0, loss 0.07832575188568847\n",
      "Epoch: 997, step: 0, loss 0.07823291263058962\n",
      "Epoch: 998, step: 0, loss 0.07814026570409681\n",
      "Epoch: 999, step: 0, loss 0.07804781054203407\n"
     ]
    }
   ],
   "source": [
    "def train() -> None:\n",
    "    ...\n",
    "\n",
    "\n",
    "train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 모델 추론 (evaluate)\n",
    "\n",
    "메인 교재와 서브 강의만을 이용하여 추론 코드를 구현하시오. (구글링 금지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.02264877] [0]\n"
     ]
    }
   ],
   "source": [
    "def test(model=model):\n",
    "    ...\n",
    "    \n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2185028b62590b62afdaea33c23835374a0efabc80ef4ce750ba82ee2e8657e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
